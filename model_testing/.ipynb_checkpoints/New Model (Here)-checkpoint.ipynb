{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/solar_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>solar_output</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>visibility</th>\n",
       "      <th>serial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>30-03-17</td>\n",
       "      <td>16:00</td>\n",
       "      <td>13.87895</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>30-03-17</td>\n",
       "      <td>16:30</td>\n",
       "      <td>11.04429</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.79</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>30-03-17</td>\n",
       "      <td>17:00</td>\n",
       "      <td>7.60269</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.10</td>\n",
       "      <td>6.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>30-03-17</td>\n",
       "      <td>17:30</td>\n",
       "      <td>4.16130</td>\n",
       "      <td>39.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>30-03-17</td>\n",
       "      <td>18:00</td>\n",
       "      <td>1.59809</td>\n",
       "      <td>39.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.30</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date   Time  solar_output  temperature  humidity  wind_speed  \\\n",
       "4495  30-03-17  16:00      13.87895         40.0       6.0       13.00   \n",
       "4496  30-03-17  16:30      11.04429         40.0       6.0       11.79   \n",
       "4497  30-03-17  17:00       7.60269         40.0       6.0       11.10   \n",
       "4498  30-03-17  17:30       4.16130         39.0       4.0       11.10   \n",
       "4499  30-03-17  18:00       1.59809         39.0       6.0        9.30   \n",
       "\n",
       "      visibility  serial  \n",
       "4495         6.0      21  \n",
       "4496         6.0      22  \n",
       "4497         6.0      23  \n",
       "4498         4.0      24  \n",
       "4499         6.0      25  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date             object\n",
       "Time             object\n",
       "solar_output    float64\n",
       "temperature     float64\n",
       "humidity        float64\n",
       "wind_speed      float64\n",
       "visibility      float64\n",
       "serial            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TimeStamp']=pd.to_datetime(data.Date+' '+data.Time,infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['month']=data.Date.str.slice(-5,-3).astype(int).head() #String Slicing\n",
    "#data['month']=pd.to_datetime(data.Date,dayfirst=True).dt.month\n",
    "#data['Day']=pd.to_datetime(data.Date,dayfirst=True).dt.day\n",
    "data['DayOfYear']=pd.to_datetime(data.Date,dayfirst=True).dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>solar_output</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>visibility</th>\n",
       "      <th>serial</th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>DayOfYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-10-16</td>\n",
       "      <td>6:00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-10 06:00:00</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-10-16</td>\n",
       "      <td>6:30</td>\n",
       "      <td>0.14216</td>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-10 06:30:00</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-10-16</td>\n",
       "      <td>7:00</td>\n",
       "      <td>1.34680</td>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-01-10 07:00:00</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-10-16</td>\n",
       "      <td>7:30</td>\n",
       "      <td>3.37716</td>\n",
       "      <td>28.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-01-10 07:30:00</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-10-16</td>\n",
       "      <td>8:00</td>\n",
       "      <td>5.67044</td>\n",
       "      <td>28.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-10 08:00:00</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date  Time  solar_output  temperature  humidity  wind_speed  \\\n",
       "0  01-10-16  6:00       0.00000         27.0      74.0         0.0   \n",
       "1  01-10-16  6:30       0.14216         27.0      74.0         1.9   \n",
       "2  01-10-16  7:00       1.34680         27.0      74.0         0.0   \n",
       "3  01-10-16  7:30       3.37716         28.0      70.0         7.4   \n",
       "4  01-10-16  8:00       5.67044         28.0      70.0         9.3   \n",
       "\n",
       "   visibility  serial           TimeStamp  DayOfYear  \n",
       "0         4.0       1 2016-01-10 06:00:00        275  \n",
       "1         4.0       2 2016-01-10 06:30:00        275  \n",
       "2         4.0       3 2016-01-10 07:00:00        275  \n",
       "3         4.0       4 2016-01-10 07:30:00        275  \n",
       "4         4.0       5 2016-01-10 08:00:00        275  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "#data.to_csv('data_seperated.csv',ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['Date','Time','TimeStamp'],axis=1)\n",
    "X = data.iloc[:,1:]\n",
    "Y=data['solar_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'serial':'TimeOfDay'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28a58972588>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(data.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "corr = data.corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n",
    "plt.savefig('hello.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_hrs = 1\n",
    "shift_steps = shift_hrs*2\n",
    "\n",
    "\n",
    "\n",
    "Y=data['solar_output'].shift(-shift_steps)\n",
    "X=data.drop(['solar_output'],axis=1)\n",
    "\n",
    "x_data = X.values[0:-shift_steps]\n",
    "y_data = Y.values[0:-shift_steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4498,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X=X.values\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Selection \n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "#X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    "#estimator = SVR(kernel=\"linear\")\n",
    "selector = RFE(rf, step=1)\n",
    "selector = selector.fit(X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 4, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>visibility</th>\n",
       "      <th>serial</th>\n",
       "      <th>DayOfYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   temperature  humidity  wind_speed  visibility  serial  DayOfYear\n",
       "0         27.0      74.0         0.0         4.0       1        275\n",
       "1         27.0      74.0         1.9         4.0       2        275\n",
       "2         27.0      74.0         0.0         4.0       3        275\n",
       "3         28.0      70.0         7.4         4.0       4        275\n",
       "4         28.0      70.0         9.3         4.0       5        275"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "x_data_scaled=x_scaler.fit_transform(x_data)\n",
    "#Y=(Y-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63888889, 0.72916667, 0.0823922 , 0.48192771, 0.        ,\n",
       "        0.75068493],\n",
       "       [0.63888889, 0.72916667, 0.13116016, 0.48192771, 0.04166667,\n",
       "        0.75068493],\n",
       "       [0.63888889, 0.72916667, 0.0823922 , 0.48192771, 0.08333333,\n",
       "        0.75068493],\n",
       "       ...,\n",
       "       [1.        , 0.02083333, 0.41606776, 0.72289157, 0.83333333,\n",
       "        0.24109589],\n",
       "       [1.        , 0.02083333, 0.38501027, 0.72289157, 0.875     ,\n",
       "        0.24109589],\n",
       "       [1.        , 0.02083333, 0.36729979, 0.72289157, 0.91666667,\n",
       "        0.24109589]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data_scaled, y_data, test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               896       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 67,073\n",
      "Trainable params: 67,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu,\n",
    "                       input_shape=(X.shape[1],)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    \n",
    "    \n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.train.RMSPropOptimizer(0.001)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae'])\n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2518 samples, validate on 630 samples\n",
      "Epoch 1/500\n",
      "2518/2518 [==============================] - 1s 245us/step - loss: 6.4899 - acc: 0.1112 - val_loss: 8.4921 - val_acc: 0.1032\n",
      "Epoch 2/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 6.5557 - acc: 0.1112 - val_loss: 9.2399 - val_acc: 0.0968\n",
      "Epoch 3/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 6.5827 - acc: 0.1112 - val_loss: 9.1915 - val_acc: 0.1032\n",
      "Epoch 4/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 6.7212 - acc: 0.1108 - val_loss: 8.6348 - val_acc: 0.1016\n",
      "Epoch 5/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 6.5372 - acc: 0.1108 - val_loss: 12.5620 - val_acc: 0.1016\n",
      "Epoch 6/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 6.6339 - acc: 0.1100 - val_loss: 10.2825 - val_acc: 0.1032\n",
      "Epoch 7/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 6.3204 - acc: 0.1116 - val_loss: 9.4757 - val_acc: 0.1016\n",
      "Epoch 8/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 6.5291 - acc: 0.1112 - val_loss: 8.9178 - val_acc: 0.1032\n",
      "Epoch 9/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 6.3799 - acc: 0.1112 - val_loss: 9.8058 - val_acc: 0.1032\n",
      "Epoch 10/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 6.2717 - acc: 0.1108 - val_loss: 9.2281 - val_acc: 0.1032\n",
      "Epoch 11/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 6.3612 - acc: 0.1108 - val_loss: 8.7608 - val_acc: 0.1032\n",
      "Epoch 12/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 6.3222 - acc: 0.1108 - val_loss: 7.9963 - val_acc: 0.1032\n",
      "Epoch 13/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 6.4199 - acc: 0.1100 - val_loss: 10.3629 - val_acc: 0.1032\n",
      "Epoch 14/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 6.2522 - acc: 0.1096 - val_loss: 8.5785 - val_acc: 0.1000\n",
      "Epoch 15/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 6.1717 - acc: 0.1112 - val_loss: 12.5711 - val_acc: 0.1048\n",
      "Epoch 16/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 6.0371 - acc: 0.1096 - val_loss: 8.7220 - val_acc: 0.1032\n",
      "Epoch 17/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 6.0537 - acc: 0.1120 - val_loss: 10.0915 - val_acc: 0.1032\n",
      "Epoch 18/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 6.2066 - acc: 0.1116 - val_loss: 8.6543 - val_acc: 0.1016\n",
      "Epoch 19/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 6.1453 - acc: 0.1116 - val_loss: 8.0106 - val_acc: 0.1032\n",
      "Epoch 20/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 6.0277 - acc: 0.1116 - val_loss: 10.1778 - val_acc: 0.1032\n",
      "Epoch 21/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 6.0494 - acc: 0.1124 - val_loss: 8.3791 - val_acc: 0.1032\n",
      "Epoch 22/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 6.0075 - acc: 0.1116 - val_loss: 10.4490 - val_acc: 0.0889\n",
      "Epoch 23/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 6.0372 - acc: 0.1120 - val_loss: 9.3724 - val_acc: 0.1016\n",
      "Epoch 24/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 6.0021 - acc: 0.1112 - val_loss: 9.5229 - val_acc: 0.1048\n",
      "Epoch 25/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 5.9511 - acc: 0.1116 - val_loss: 8.0696 - val_acc: 0.1032\n",
      "Epoch 26/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 5.8127 - acc: 0.1128 - val_loss: 7.5616 - val_acc: 0.1032\n",
      "Epoch 27/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 5.8721 - acc: 0.1116 - val_loss: 12.4221 - val_acc: 0.1032\n",
      "Epoch 28/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 5.8024 - acc: 0.1124 - val_loss: 8.8183 - val_acc: 0.1032\n",
      "Epoch 29/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 5.8655 - acc: 0.1120 - val_loss: 8.7770 - val_acc: 0.1016\n",
      "Epoch 30/500\n",
      "2518/2518 [==============================] - 0s 103us/step - loss: 5.7891 - acc: 0.1112 - val_loss: 8.4078 - val_acc: 0.1032\n",
      "Epoch 31/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 5.7040 - acc: 0.1120 - val_loss: 8.9109 - val_acc: 0.0905\n",
      "Epoch 32/500\n",
      "2518/2518 [==============================] - 0s 106us/step - loss: 5.5680 - acc: 0.1112 - val_loss: 9.5789 - val_acc: 0.0937\n",
      "Epoch 33/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 5.4886 - acc: 0.1108 - val_loss: 9.4907 - val_acc: 0.1032\n",
      "Epoch 34/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 5.6040 - acc: 0.1108 - val_loss: 8.3472 - val_acc: 0.1032\n",
      "Epoch 35/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 5.3836 - acc: 0.1112 - val_loss: 8.3451 - val_acc: 0.1032\n",
      "Epoch 36/500\n",
      "2518/2518 [==============================] - 0s 111us/step - loss: 5.5567 - acc: 0.1136 - val_loss: 8.7422 - val_acc: 0.1032\n",
      "Epoch 37/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 5.5756 - acc: 0.1124 - val_loss: 8.8801 - val_acc: 0.1032\n",
      "Epoch 38/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 5.4891 - acc: 0.1100 - val_loss: 9.1119 - val_acc: 0.1016\n",
      "Epoch 39/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 5.5601 - acc: 0.1116 - val_loss: 7.6480 - val_acc: 0.1032\n",
      "Epoch 40/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 5.5831 - acc: 0.1120 - val_loss: 7.8945 - val_acc: 0.1032\n",
      "Epoch 41/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 5.2445 - acc: 0.1120 - val_loss: 10.2497 - val_acc: 0.1032\n",
      "Epoch 42/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 5.4342 - acc: 0.1124 - val_loss: 8.0496 - val_acc: 0.1032\n",
      "Epoch 43/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 5.4224 - acc: 0.1128 - val_loss: 8.6919 - val_acc: 0.1032\n",
      "Epoch 44/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 5.3294 - acc: 0.1104 - val_loss: 8.0747 - val_acc: 0.1032\n",
      "Epoch 45/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 5.1022 - acc: 0.1116 - val_loss: 7.9517 - val_acc: 0.0984\n",
      "Epoch 46/500\n",
      "2518/2518 [==============================] - 0s 89us/step - loss: 5.0981 - acc: 0.1112 - val_loss: 8.4445 - val_acc: 0.1032\n",
      "Epoch 47/500\n",
      "2518/2518 [==============================] - 0s 104us/step - loss: 5.1232 - acc: 0.1120 - val_loss: 8.0054 - val_acc: 0.1032\n",
      "Epoch 48/500\n",
      "2518/2518 [==============================] - 0s 106us/step - loss: 5.1877 - acc: 0.1120 - val_loss: 7.7895 - val_acc: 0.1032\n",
      "Epoch 49/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 5.2656 - acc: 0.1120 - val_loss: 10.0854 - val_acc: 0.1032\n",
      "Epoch 50/500\n",
      "2518/2518 [==============================] - 0s 107us/step - loss: 5.1244 - acc: 0.1112 - val_loss: 7.9212 - val_acc: 0.1032\n",
      "Epoch 51/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 5.0394 - acc: 0.1116 - val_loss: 7.8054 - val_acc: 0.1032\n",
      "Epoch 52/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 5.2452 - acc: 0.1112 - val_loss: 9.5660 - val_acc: 0.1032\n",
      "Epoch 53/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 4.7462 - acc: 0.1128 - val_loss: 8.2555 - val_acc: 0.1032\n",
      "Epoch 54/500\n",
      "2518/2518 [==============================] - 0s 89us/step - loss: 4.9964 - acc: 0.1124 - val_loss: 8.8256 - val_acc: 0.0905\n",
      "Epoch 55/500\n",
      "2518/2518 [==============================] - 0s 86us/step - loss: 5.1505 - acc: 0.1104 - val_loss: 10.2892 - val_acc: 0.1032\n",
      "Epoch 56/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 4.8949 - acc: 0.1124 - val_loss: 8.1043 - val_acc: 0.0984\n",
      "Epoch 57/500\n",
      "2518/2518 [==============================] - 0s 107us/step - loss: 5.0168 - acc: 0.1112 - val_loss: 10.0866 - val_acc: 0.1032\n",
      "Epoch 58/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 4.9649 - acc: 0.1128 - val_loss: 7.1427 - val_acc: 0.1016\n",
      "Epoch 59/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 4.9363 - acc: 0.1124 - val_loss: 8.5759 - val_acc: 0.1000\n",
      "Epoch 60/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518/2518 [==============================] - 0s 98us/step - loss: 4.6548 - acc: 0.1128 - val_loss: 7.5399 - val_acc: 0.1032\n",
      "Epoch 61/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 4.8255 - acc: 0.1120 - val_loss: 7.5531 - val_acc: 0.1032\n",
      "Epoch 62/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 4.6589 - acc: 0.1124 - val_loss: 7.2718 - val_acc: 0.0937\n",
      "Epoch 63/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 4.6171 - acc: 0.1108 - val_loss: 7.9080 - val_acc: 0.1032\n",
      "Epoch 64/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 4.8161 - acc: 0.1120 - val_loss: 7.5452 - val_acc: 0.1032\n",
      "Epoch 65/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 4.5565 - acc: 0.1124 - val_loss: 7.6517 - val_acc: 0.1032\n",
      "Epoch 66/500\n",
      "2518/2518 [==============================] - 0s 89us/step - loss: 4.4404 - acc: 0.1128 - val_loss: 7.5565 - val_acc: 0.1032\n",
      "Epoch 67/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 4.6475 - acc: 0.1128 - val_loss: 7.7908 - val_acc: 0.1032\n",
      "Epoch 68/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 4.4574 - acc: 0.1128 - val_loss: 7.8446 - val_acc: 0.1032\n",
      "Epoch 69/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 4.6785 - acc: 0.1124 - val_loss: 11.9877 - val_acc: 0.1063\n",
      "Epoch 70/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 4.6381 - acc: 0.1120 - val_loss: 8.1381 - val_acc: 0.1032\n",
      "Epoch 71/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 4.4909 - acc: 0.1136 - val_loss: 7.4275 - val_acc: 0.1032\n",
      "Epoch 72/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 4.3882 - acc: 0.1120 - val_loss: 7.2913 - val_acc: 0.1048\n",
      "Epoch 73/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 4.4259 - acc: 0.1124 - val_loss: 8.2736 - val_acc: 0.1032\n",
      "Epoch 74/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 4.2465 - acc: 0.1132 - val_loss: 8.4443 - val_acc: 0.0984\n",
      "Epoch 75/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 4.3862 - acc: 0.1128 - val_loss: 7.0592 - val_acc: 0.1063\n",
      "Epoch 76/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 4.1174 - acc: 0.1148 - val_loss: 7.9694 - val_acc: 0.1032\n",
      "Epoch 77/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 4.4151 - acc: 0.1128 - val_loss: 7.4513 - val_acc: 0.1032\n",
      "Epoch 78/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 4.2864 - acc: 0.1128 - val_loss: 6.8259 - val_acc: 0.1032\n",
      "Epoch 79/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 4.0377 - acc: 0.1128 - val_loss: 8.5009 - val_acc: 0.1000\n",
      "Epoch 80/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 4.1954 - acc: 0.1120 - val_loss: 7.4076 - val_acc: 0.1048\n",
      "Epoch 81/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 4.0725 - acc: 0.1124 - val_loss: 7.1948 - val_acc: 0.1032\n",
      "Epoch 82/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 4.2108 - acc: 0.1136 - val_loss: 7.8491 - val_acc: 0.1032\n",
      "Epoch 83/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 4.0815 - acc: 0.1128 - val_loss: 7.1049 - val_acc: 0.1032\n",
      "Epoch 84/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 4.1020 - acc: 0.1120 - val_loss: 7.4087 - val_acc: 0.1032\n",
      "Epoch 85/500\n",
      "2518/2518 [==============================] - 0s 105us/step - loss: 4.1791 - acc: 0.1120 - val_loss: 7.8261 - val_acc: 0.1048\n",
      "Epoch 86/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 4.1618 - acc: 0.1136 - val_loss: 7.6332 - val_acc: 0.1000\n",
      "Epoch 87/500\n",
      "2518/2518 [==============================] - 0s 106us/step - loss: 4.2028 - acc: 0.1132 - val_loss: 6.8140 - val_acc: 0.1048\n",
      "Epoch 88/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 4.0306 - acc: 0.1132 - val_loss: 8.2757 - val_acc: 0.1032\n",
      "Epoch 89/500\n",
      "2518/2518 [==============================] - 0s 104us/step - loss: 3.8840 - acc: 0.1132 - val_loss: 7.9750 - val_acc: 0.1032\n",
      "Epoch 90/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 3.8591 - acc: 0.1116 - val_loss: 7.9431 - val_acc: 0.1048\n",
      "Epoch 91/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 4.0098 - acc: 0.1124 - val_loss: 8.6318 - val_acc: 0.1032\n",
      "Epoch 92/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 4.0145 - acc: 0.1132 - val_loss: 8.1966 - val_acc: 0.1032\n",
      "Epoch 93/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 3.9090 - acc: 0.1124 - val_loss: 8.2569 - val_acc: 0.1032\n",
      "Epoch 94/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 3.9352 - acc: 0.1132 - val_loss: 8.0244 - val_acc: 0.1032\n",
      "Epoch 95/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 3.8598 - acc: 0.1124 - val_loss: 8.3076 - val_acc: 0.1032\n",
      "Epoch 96/500\n",
      "2518/2518 [==============================] - 0s 106us/step - loss: 3.8838 - acc: 0.1136 - val_loss: 7.8112 - val_acc: 0.1063\n",
      "Epoch 97/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 3.9304 - acc: 0.1120 - val_loss: 9.0910 - val_acc: 0.1032\n",
      "Epoch 98/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 3.7421 - acc: 0.1136 - val_loss: 8.3462 - val_acc: 0.1032\n",
      "Epoch 99/500\n",
      "2518/2518 [==============================] - 0s 103us/step - loss: 3.8372 - acc: 0.1132 - val_loss: 7.4562 - val_acc: 0.1032\n",
      "Epoch 100/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 3.8870 - acc: 0.1140 - val_loss: 10.3831 - val_acc: 0.1032\n",
      "Epoch 101/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 3.6703 - acc: 0.1128 - val_loss: 9.0117 - val_acc: 0.1032\n",
      "Epoch 102/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 3.8153 - acc: 0.1140 - val_loss: 9.0180 - val_acc: 0.1032\n",
      "Epoch 103/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 3.7571 - acc: 0.1128 - val_loss: 7.1476 - val_acc: 0.1032\n",
      "Epoch 104/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 3.7529 - acc: 0.1136 - val_loss: 7.3657 - val_acc: 0.1063\n",
      "Epoch 105/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 3.5602 - acc: 0.1136 - val_loss: 7.4305 - val_acc: 0.1032\n",
      "Epoch 106/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 3.6792 - acc: 0.1136 - val_loss: 7.4068 - val_acc: 0.1063\n",
      "Epoch 107/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 3.6049 - acc: 0.1124 - val_loss: 7.1377 - val_acc: 0.1032\n",
      "Epoch 108/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 3.6300 - acc: 0.1132 - val_loss: 7.6741 - val_acc: 0.1048\n",
      "Epoch 109/500\n",
      "2518/2518 [==============================] - 0s 103us/step - loss: 3.5205 - acc: 0.1128 - val_loss: 7.3650 - val_acc: 0.1048\n",
      "Epoch 110/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 3.3261 - acc: 0.1144 - val_loss: 7.8885 - val_acc: 0.1032\n",
      "Epoch 111/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 3.5862 - acc: 0.1128 - val_loss: 8.5794 - val_acc: 0.1032\n",
      "Epoch 112/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 3.4252 - acc: 0.1132 - val_loss: 7.1147 - val_acc: 0.1032\n",
      "Epoch 113/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 3.3014 - acc: 0.1144 - val_loss: 8.0915 - val_acc: 0.1032\n",
      "Epoch 114/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 3.5389 - acc: 0.1128 - val_loss: 7.7291 - val_acc: 0.1032\n",
      "Epoch 115/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 3.5277 - acc: 0.1136 - val_loss: 7.3425 - val_acc: 0.1048\n",
      "Epoch 116/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 3.3969 - acc: 0.1140 - val_loss: 11.6637 - val_acc: 0.1032\n",
      "Epoch 117/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 3.5362 - acc: 0.1136 - val_loss: 8.0803 - val_acc: 0.1032\n",
      "Epoch 118/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 3.3815 - acc: 0.1132 - val_loss: 7.6087 - val_acc: 0.1000\n",
      "Epoch 119/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518/2518 [==============================] - ETA: 0s - loss: 3.2913 - acc: 0.109 - 0s 86us/step - loss: 3.5432 - acc: 0.1136 - val_loss: 7.2872 - val_acc: 0.1032\n",
      "Epoch 120/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 3.4781 - acc: 0.1132 - val_loss: 7.8958 - val_acc: 0.1000\n",
      "Epoch 121/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 3.4184 - acc: 0.1132 - val_loss: 8.2079 - val_acc: 0.1016\n",
      "Epoch 122/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 3.3991 - acc: 0.1128 - val_loss: 7.3623 - val_acc: 0.1063\n",
      "Epoch 123/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 3.1798 - acc: 0.1136 - val_loss: 7.6447 - val_acc: 0.1063\n",
      "Epoch 124/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 3.3398 - acc: 0.1136 - val_loss: 7.0579 - val_acc: 0.1048\n",
      "Epoch 125/500\n",
      "2518/2518 [==============================] - 0s 106us/step - loss: 3.2670 - acc: 0.1132 - val_loss: 7.2265 - val_acc: 0.1048\n",
      "Epoch 126/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 3.3416 - acc: 0.1124 - val_loss: 7.9660 - val_acc: 0.1000\n",
      "Epoch 127/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 3.2570 - acc: 0.1128 - val_loss: 8.3552 - val_acc: 0.1063\n",
      "Epoch 128/500\n",
      "2518/2518 [==============================] - 0s 107us/step - loss: 3.3266 - acc: 0.1140 - val_loss: 7.8295 - val_acc: 0.1048\n",
      "Epoch 129/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 3.2768 - acc: 0.1144 - val_loss: 9.0099 - val_acc: 0.1048\n",
      "Epoch 130/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 3.0285 - acc: 0.1136 - val_loss: 7.6847 - val_acc: 0.1032\n",
      "Epoch 131/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 3.2732 - acc: 0.1148 - val_loss: 8.6715 - val_acc: 0.1048\n",
      "Epoch 132/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 3.2889 - acc: 0.1140 - val_loss: 7.7092 - val_acc: 0.1032\n",
      "Epoch 133/500\n",
      "2518/2518 [==============================] - 0s 89us/step - loss: 3.1878 - acc: 0.1140 - val_loss: 7.2179 - val_acc: 0.1063\n",
      "Epoch 134/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 3.0252 - acc: 0.1136 - val_loss: 8.0948 - val_acc: 0.1048\n",
      "Epoch 135/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 3.1335 - acc: 0.1144 - val_loss: 7.3214 - val_acc: 0.1063\n",
      "Epoch 136/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 2.9455 - acc: 0.1136 - val_loss: 8.3483 - val_acc: 0.1016\n",
      "Epoch 137/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 3.1203 - acc: 0.1136 - val_loss: 7.6806 - val_acc: 0.1048\n",
      "Epoch 138/500\n",
      "2518/2518 [==============================] - 0s 105us/step - loss: 3.0869 - acc: 0.1148 - val_loss: 8.6485 - val_acc: 0.0937\n",
      "Epoch 139/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 3.1320 - acc: 0.1148 - val_loss: 7.1636 - val_acc: 0.1048\n",
      "Epoch 140/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 3.1073 - acc: 0.1128 - val_loss: 8.1958 - val_acc: 0.0921\n",
      "Epoch 141/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 3.1095 - acc: 0.1152 - val_loss: 8.1493 - val_acc: 0.1063\n",
      "Epoch 142/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 2.8559 - acc: 0.1136 - val_loss: 7.3449 - val_acc: 0.1048\n",
      "Epoch 143/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 3.0861 - acc: 0.1148 - val_loss: 9.4365 - val_acc: 0.1063\n",
      "Epoch 144/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 3.1823 - acc: 0.1144 - val_loss: 8.3858 - val_acc: 0.1048\n",
      "Epoch 145/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 2.9278 - acc: 0.1160 - val_loss: 10.0611 - val_acc: 0.1048\n",
      "Epoch 146/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 3.0960 - acc: 0.1140 - val_loss: 7.7642 - val_acc: 0.1048\n",
      "Epoch 147/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.9523 - acc: 0.1144 - val_loss: 8.7986 - val_acc: 0.1032\n",
      "Epoch 148/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 2.9347 - acc: 0.1144 - val_loss: 7.8056 - val_acc: 0.1079\n",
      "Epoch 149/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 3.0402 - acc: 0.1132 - val_loss: 8.0451 - val_acc: 0.1063\n",
      "Epoch 150/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 3.0400 - acc: 0.1148 - val_loss: 7.2528 - val_acc: 0.1063\n",
      "Epoch 151/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.8545 - acc: 0.1144 - val_loss: 9.9949 - val_acc: 0.1048\n",
      "Epoch 152/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 2.9672 - acc: 0.1156 - val_loss: 7.8422 - val_acc: 0.1063\n",
      "Epoch 153/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 2.7952 - acc: 0.1148 - val_loss: 7.1621 - val_acc: 0.1032\n",
      "Epoch 154/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 2.9011 - acc: 0.1140 - val_loss: 8.4716 - val_acc: 0.1032\n",
      "Epoch 155/500\n",
      "2518/2518 [==============================] - 0s 104us/step - loss: 3.0464 - acc: 0.1152 - val_loss: 7.9367 - val_acc: 0.1048\n",
      "Epoch 156/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 2.8538 - acc: 0.1152 - val_loss: 7.5424 - val_acc: 0.1063\n",
      "Epoch 157/500\n",
      "2518/2518 [==============================] - 0s 104us/step - loss: 2.7338 - acc: 0.1152 - val_loss: 8.5150 - val_acc: 0.1032\n",
      "Epoch 158/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 2.7664 - acc: 0.1156 - val_loss: 7.6214 - val_acc: 0.1048\n",
      "Epoch 159/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 2.8798 - acc: 0.1144 - val_loss: 9.1565 - val_acc: 0.1063\n",
      "Epoch 160/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 2.8474 - acc: 0.1148 - val_loss: 7.6929 - val_acc: 0.1063\n",
      "Epoch 161/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 2.7256 - acc: 0.1168 - val_loss: 8.5983 - val_acc: 0.1032\n",
      "Epoch 162/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.6346 - acc: 0.1136 - val_loss: 9.1409 - val_acc: 0.1079\n",
      "Epoch 163/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.6928 - acc: 0.1148 - val_loss: 8.2954 - val_acc: 0.1048\n",
      "Epoch 164/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 2.8243 - acc: 0.1156 - val_loss: 8.2568 - val_acc: 0.1032\n",
      "Epoch 165/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 2.5577 - acc: 0.1140 - val_loss: 8.8918 - val_acc: 0.1063\n",
      "Epoch 166/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.6508 - acc: 0.1160 - val_loss: 7.8751 - val_acc: 0.1063\n",
      "Epoch 167/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 2.6941 - acc: 0.1140 - val_loss: 8.6370 - val_acc: 0.1048\n",
      "Epoch 168/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 2.6834 - acc: 0.1144 - val_loss: 8.2213 - val_acc: 0.1063\n",
      "Epoch 169/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.8505 - acc: 0.1152 - val_loss: 7.8160 - val_acc: 0.1079\n",
      "Epoch 170/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 2.7333 - acc: 0.1152 - val_loss: 8.2567 - val_acc: 0.1032\n",
      "Epoch 171/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.4812 - acc: 0.1144 - val_loss: 9.2827 - val_acc: 0.1032\n",
      "Epoch 172/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 2.6856 - acc: 0.1140 - val_loss: 7.7770 - val_acc: 0.1063\n",
      "Epoch 173/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 2.6710 - acc: 0.1140 - val_loss: 8.6305 - val_acc: 0.1048\n",
      "Epoch 174/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 2.6535 - acc: 0.1148 - val_loss: 7.5106 - val_acc: 0.1079\n",
      "Epoch 175/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 2.5538 - acc: 0.1152 - val_loss: 8.8313 - val_acc: 0.1048\n",
      "Epoch 176/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 2.6232 - acc: 0.1148 - val_loss: 8.0876 - val_acc: 0.1048\n",
      "Epoch 177/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 2.4582 - acc: 0.1152 - val_loss: 7.8492 - val_acc: 0.1063\n",
      "Epoch 178/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518/2518 [==============================] - 0s 91us/step - loss: 2.5469 - acc: 0.1156 - val_loss: 7.7719 - val_acc: 0.1048\n",
      "Epoch 179/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 2.6247 - acc: 0.1152 - val_loss: 12.4651 - val_acc: 0.1063\n",
      "Epoch 180/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 2.8208 - acc: 0.1160 - val_loss: 10.1805 - val_acc: 0.1016\n",
      "Epoch 181/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 2.5716 - acc: 0.1164 - val_loss: 7.4478 - val_acc: 0.1079\n",
      "Epoch 182/500\n",
      "2518/2518 [==============================] - 0s 84us/step - loss: 2.4824 - acc: 0.1156 - val_loss: 7.8376 - val_acc: 0.1048\n",
      "Epoch 183/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 2.4086 - acc: 0.1140 - val_loss: 7.9858 - val_acc: 0.1079\n",
      "Epoch 184/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 2.5032 - acc: 0.1148 - val_loss: 8.3007 - val_acc: 0.1079\n",
      "Epoch 185/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 2.3798 - acc: 0.1160 - val_loss: 8.5056 - val_acc: 0.1079\n",
      "Epoch 186/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 2.4351 - acc: 0.1144 - val_loss: 7.9235 - val_acc: 0.1079\n",
      "Epoch 187/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 2.7626 - acc: 0.1152 - val_loss: 9.9622 - val_acc: 0.1048\n",
      "Epoch 188/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 2.6191 - acc: 0.1140 - val_loss: 8.0999 - val_acc: 0.1079\n",
      "Epoch 189/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 2.6354 - acc: 0.1172 - val_loss: 7.6986 - val_acc: 0.1063\n",
      "Epoch 190/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 2.5938 - acc: 0.1144 - val_loss: 8.1602 - val_acc: 0.1032\n",
      "Epoch 191/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 2.5995 - acc: 0.1152 - val_loss: 7.8462 - val_acc: 0.1063\n",
      "Epoch 192/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 2.4445 - acc: 0.1156 - val_loss: 8.5148 - val_acc: 0.1048\n",
      "Epoch 193/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 2.3721 - acc: 0.1168 - val_loss: 10.1891 - val_acc: 0.1079\n",
      "Epoch 194/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 2.3086 - acc: 0.1172 - val_loss: 9.1813 - val_acc: 0.1048\n",
      "Epoch 195/500\n",
      "2518/2518 [==============================] - 0s 89us/step - loss: 2.4367 - acc: 0.1144 - val_loss: 8.1894 - val_acc: 0.1079\n",
      "Epoch 196/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 2.4341 - acc: 0.1164 - val_loss: 8.7970 - val_acc: 0.1048\n",
      "Epoch 197/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 2.4180 - acc: 0.1164 - val_loss: 7.7736 - val_acc: 0.1079\n",
      "Epoch 198/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 2.4854 - acc: 0.1152 - val_loss: 8.2792 - val_acc: 0.1063\n",
      "Epoch 199/500\n",
      "2518/2518 [==============================] - 0s 89us/step - loss: 2.3762 - acc: 0.1156 - val_loss: 7.8683 - val_acc: 0.1032\n",
      "Epoch 200/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 2.5607 - acc: 0.1156 - val_loss: 8.0942 - val_acc: 0.1079\n",
      "Epoch 201/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 2.3108 - acc: 0.1152 - val_loss: 8.9608 - val_acc: 0.1032\n",
      "Epoch 202/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 2.3258 - acc: 0.1148 - val_loss: 9.3723 - val_acc: 0.1048\n",
      "Epoch 203/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.6416 - acc: 0.1164 - val_loss: 7.9047 - val_acc: 0.1016\n",
      "Epoch 204/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.4042 - acc: 0.1148 - val_loss: 8.3761 - val_acc: 0.1095\n",
      "Epoch 205/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 2.6413 - acc: 0.1164 - val_loss: 7.5147 - val_acc: 0.1079\n",
      "Epoch 206/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 2.3046 - acc: 0.1180 - val_loss: 9.1312 - val_acc: 0.1063\n",
      "Epoch 207/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.0680 - acc: 0.1144 - val_loss: 9.2306 - val_acc: 0.1063\n",
      "Epoch 208/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 2.4376 - acc: 0.1164 - val_loss: 8.1492 - val_acc: 0.1063\n",
      "Epoch 209/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 2.1132 - acc: 0.1164 - val_loss: 8.0590 - val_acc: 0.1048\n",
      "Epoch 210/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 2.1184 - acc: 0.1160 - val_loss: 8.3119 - val_acc: 0.1032\n",
      "Epoch 211/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 2.2892 - acc: 0.1176 - val_loss: 8.0502 - val_acc: 0.1063\n",
      "Epoch 212/500\n",
      "2518/2518 [==============================] - 0s 103us/step - loss: 2.2633 - acc: 0.1164 - val_loss: 7.9130 - val_acc: 0.1063\n",
      "Epoch 213/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 2.3427 - acc: 0.1156 - val_loss: 8.0759 - val_acc: 0.1063\n",
      "Epoch 214/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 2.1500 - acc: 0.1156 - val_loss: 7.4503 - val_acc: 0.1032\n",
      "Epoch 215/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 2.1967 - acc: 0.1168 - val_loss: 9.7477 - val_acc: 0.1079\n",
      "Epoch 216/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 2.1692 - acc: 0.1164 - val_loss: 9.2988 - val_acc: 0.1079\n",
      "Epoch 217/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.2138 - acc: 0.1148 - val_loss: 7.5148 - val_acc: 0.1063\n",
      "Epoch 218/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.0889 - acc: 0.1156 - val_loss: 7.8160 - val_acc: 0.1048\n",
      "Epoch 219/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 2.1052 - acc: 0.1156 - val_loss: 7.8948 - val_acc: 0.1063\n",
      "Epoch 220/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 2.2281 - acc: 0.1152 - val_loss: 9.8071 - val_acc: 0.1048\n",
      "Epoch 221/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.0624 - acc: 0.1164 - val_loss: 8.5223 - val_acc: 0.1063\n",
      "Epoch 222/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 2.2120 - acc: 0.1160 - val_loss: 9.5337 - val_acc: 0.1048\n",
      "Epoch 223/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 2.1919 - acc: 0.1160 - val_loss: 7.6127 - val_acc: 0.1063\n",
      "Epoch 224/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 2.2603 - acc: 0.1156 - val_loss: 8.3342 - val_acc: 0.1063\n",
      "Epoch 225/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.0513 - acc: 0.1156 - val_loss: 10.8404 - val_acc: 0.1063\n",
      "Epoch 226/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 2.0317 - acc: 0.1152 - val_loss: 8.1420 - val_acc: 0.1032\n",
      "Epoch 227/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 2.1697 - acc: 0.1168 - val_loss: 7.7013 - val_acc: 0.1016\n",
      "Epoch 228/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 2.0862 - acc: 0.1160 - val_loss: 8.3014 - val_acc: 0.1063\n",
      "Epoch 229/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 2.0885 - acc: 0.1180 - val_loss: 7.7928 - val_acc: 0.1032\n",
      "Epoch 230/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 2.1010 - acc: 0.1156 - val_loss: 8.1600 - val_acc: 0.1063\n",
      "Epoch 231/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 2.2137 - acc: 0.1164 - val_loss: 9.7766 - val_acc: 0.1063\n",
      "Epoch 232/500\n",
      "2518/2518 [==============================] - 0s 104us/step - loss: 2.2335 - acc: 0.1156 - val_loss: 12.1194 - val_acc: 0.1016\n",
      "Epoch 233/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 2.0218 - acc: 0.1164 - val_loss: 10.2767 - val_acc: 0.1095\n",
      "Epoch 234/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 2.1114 - acc: 0.1160 - val_loss: 7.8158 - val_acc: 0.1048\n",
      "Epoch 235/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 1.9244 - acc: 0.1164 - val_loss: 9.7817 - val_acc: 0.1048\n",
      "Epoch 236/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 2.1718 - acc: 0.1168 - val_loss: 7.8118 - val_acc: 0.1048\n",
      "Epoch 237/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518/2518 [==============================] - 0s 95us/step - loss: 2.3568 - acc: 0.1180 - val_loss: 8.0653 - val_acc: 0.1079\n",
      "Epoch 238/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 1.7751 - acc: 0.1156 - val_loss: 9.1393 - val_acc: 0.1063\n",
      "Epoch 239/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 2.0010 - acc: 0.1168 - val_loss: 8.4317 - val_acc: 0.1079\n",
      "Epoch 240/500\n",
      "2518/2518 [==============================] - 0s 84us/step - loss: 1.9299 - acc: 0.1152 - val_loss: 9.7996 - val_acc: 0.1048\n",
      "Epoch 241/500\n",
      "2518/2518 [==============================] - 0s 86us/step - loss: 2.0222 - acc: 0.1180 - val_loss: 8.0415 - val_acc: 0.1079\n",
      "Epoch 242/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 1.9521 - acc: 0.1168 - val_loss: 8.0172 - val_acc: 0.1079\n",
      "Epoch 243/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 1.8987 - acc: 0.1172 - val_loss: 9.8823 - val_acc: 0.1016\n",
      "Epoch 244/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 1.7723 - acc: 0.1160 - val_loss: 8.1588 - val_acc: 0.1063\n",
      "Epoch 245/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 1.9680 - acc: 0.1160 - val_loss: 9.8756 - val_acc: 0.1063\n",
      "Epoch 246/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 1.9320 - acc: 0.1168 - val_loss: 8.5033 - val_acc: 0.1048\n",
      "Epoch 247/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 2.0250 - acc: 0.1172 - val_loss: 8.7836 - val_acc: 0.1079\n",
      "Epoch 248/500\n",
      "2518/2518 [==============================] - 0s 89us/step - loss: 2.1819 - acc: 0.1156 - val_loss: 9.5434 - val_acc: 0.1063\n",
      "Epoch 249/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 1.9464 - acc: 0.1168 - val_loss: 7.8121 - val_acc: 0.1063\n",
      "Epoch 250/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 1.7165 - acc: 0.1168 - val_loss: 7.9865 - val_acc: 0.1079\n",
      "Epoch 251/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 1.9059 - acc: 0.1164 - val_loss: 7.1907 - val_acc: 0.1063\n",
      "Epoch 252/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 1.9075 - acc: 0.1168 - val_loss: 8.7602 - val_acc: 0.1032\n",
      "Epoch 253/500\n",
      "2518/2518 [==============================] - 0s 86us/step - loss: 2.0710 - acc: 0.1168 - val_loss: 7.2270 - val_acc: 0.1048\n",
      "Epoch 254/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 1.9490 - acc: 0.1160 - val_loss: 9.6537 - val_acc: 0.1063\n",
      "Epoch 255/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 1.9618 - acc: 0.1168 - val_loss: 10.4467 - val_acc: 0.1016\n",
      "Epoch 256/500\n",
      "2518/2518 [==============================] - 0s 89us/step - loss: 1.8156 - acc: 0.1152 - val_loss: 8.1652 - val_acc: 0.1048\n",
      "Epoch 257/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 1.9580 - acc: 0.1164 - val_loss: 10.0293 - val_acc: 0.1032\n",
      "Epoch 258/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 1.9405 - acc: 0.1156 - val_loss: 7.8028 - val_acc: 0.1048\n",
      "Epoch 259/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 1.8913 - acc: 0.1152 - val_loss: 12.3845 - val_acc: 0.1063\n",
      "Epoch 260/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.7699 - acc: 0.1164 - val_loss: 6.9952 - val_acc: 0.1032\n",
      "Epoch 261/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 2.0736 - acc: 0.1160 - val_loss: 8.9022 - val_acc: 0.1048\n",
      "Epoch 262/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 2.1648 - acc: 0.1160 - val_loss: 8.6018 - val_acc: 0.1063\n",
      "Epoch 263/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 1.8797 - acc: 0.1160 - val_loss: 7.2584 - val_acc: 0.1016\n",
      "Epoch 264/500\n",
      "2518/2518 [==============================] - 0s 89us/step - loss: 2.0221 - acc: 0.1160 - val_loss: 7.9658 - val_acc: 0.1048\n",
      "Epoch 265/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 1.9826 - acc: 0.1156 - val_loss: 8.1394 - val_acc: 0.1032\n",
      "Epoch 266/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 1.6855 - acc: 0.1168 - val_loss: 11.7612 - val_acc: 0.1032\n",
      "Epoch 267/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 1.9276 - acc: 0.1152 - val_loss: 8.3877 - val_acc: 0.1032\n",
      "Epoch 268/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 1.7487 - acc: 0.1172 - val_loss: 10.0474 - val_acc: 0.1032\n",
      "Epoch 269/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 1.8424 - acc: 0.1160 - val_loss: 8.4251 - val_acc: 0.1079\n",
      "Epoch 270/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 1.8663 - acc: 0.1164 - val_loss: 8.4061 - val_acc: 0.1079\n",
      "Epoch 271/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 1.9190 - acc: 0.1164 - val_loss: 8.1121 - val_acc: 0.1048\n",
      "Epoch 272/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.9054 - acc: 0.1176 - val_loss: 9.4064 - val_acc: 0.1016\n",
      "Epoch 273/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 1.8463 - acc: 0.1164 - val_loss: 7.6170 - val_acc: 0.1063\n",
      "Epoch 274/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.7156 - acc: 0.1176 - val_loss: 10.5863 - val_acc: 0.1063\n",
      "Epoch 275/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.6620 - acc: 0.1164 - val_loss: 8.6617 - val_acc: 0.1048\n",
      "Epoch 276/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.8359 - acc: 0.1160 - val_loss: 8.0150 - val_acc: 0.1048\n",
      "Epoch 277/500\n",
      "2518/2518 [==============================] - 0s 86us/step - loss: 1.7119 - acc: 0.1172 - val_loss: 7.5099 - val_acc: 0.1016\n",
      "Epoch 278/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 1.7022 - acc: 0.1160 - val_loss: 8.0393 - val_acc: 0.1032\n",
      "Epoch 279/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 1.6912 - acc: 0.1168 - val_loss: 7.7564 - val_acc: 0.1032\n",
      "Epoch 280/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.6952 - acc: 0.1160 - val_loss: 8.4353 - val_acc: 0.1032\n",
      "Epoch 281/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.8287 - acc: 0.1168 - val_loss: 8.3663 - val_acc: 0.1063\n",
      "Epoch 282/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.7304 - acc: 0.1156 - val_loss: 9.2987 - val_acc: 0.1032\n",
      "Epoch 283/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.7405 - acc: 0.1168 - val_loss: 8.5400 - val_acc: 0.1063\n",
      "Epoch 284/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.6420 - acc: 0.1160 - val_loss: 8.6599 - val_acc: 0.1016\n",
      "Epoch 285/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 1.8393 - acc: 0.1180 - val_loss: 8.7195 - val_acc: 0.1032\n",
      "Epoch 286/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.7074 - acc: 0.1187 - val_loss: 7.8220 - val_acc: 0.1063\n",
      "Epoch 287/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 1.6388 - acc: 0.1164 - val_loss: 8.3931 - val_acc: 0.1063\n",
      "Epoch 288/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 1.9439 - acc: 0.1156 - val_loss: 8.7733 - val_acc: 0.1048\n",
      "Epoch 289/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 1.6684 - acc: 0.1180 - val_loss: 7.6593 - val_acc: 0.1016\n",
      "Epoch 290/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 1.6976 - acc: 0.1160 - val_loss: 7.7293 - val_acc: 0.1032\n",
      "Epoch 291/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.7975 - acc: 0.1164 - val_loss: 9.0948 - val_acc: 0.1048\n",
      "Epoch 292/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.4895 - acc: 0.1168 - val_loss: 8.0779 - val_acc: 0.1063\n",
      "Epoch 293/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.6609 - acc: 0.1172 - val_loss: 7.7618 - val_acc: 0.1016\n",
      "Epoch 294/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 1.5622 - acc: 0.1168 - val_loss: 9.6592 - val_acc: 0.1048\n",
      "Epoch 295/500\n",
      "2518/2518 [==============================] - 0s 86us/step - loss: 2.1204 - acc: 0.1156 - val_loss: 7.4895 - val_acc: 0.1079\n",
      "Epoch 296/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518/2518 [==============================] - 0s 96us/step - loss: 1.6338 - acc: 0.1168 - val_loss: 8.6417 - val_acc: 0.1048\n",
      "Epoch 297/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 1.8690 - acc: 0.1180 - val_loss: 8.7803 - val_acc: 0.1032\n",
      "Epoch 298/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 1.6877 - acc: 0.1156 - val_loss: 8.6257 - val_acc: 0.1048\n",
      "Epoch 299/500\n",
      "2518/2518 [==============================] - 0s 84us/step - loss: 1.7757 - acc: 0.1160 - val_loss: 7.6038 - val_acc: 0.1079\n",
      "Epoch 300/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 1.6215 - acc: 0.1180 - val_loss: 9.0302 - val_acc: 0.1063\n",
      "Epoch 301/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 1.6705 - acc: 0.1176 - val_loss: 8.5428 - val_acc: 0.1079\n",
      "Epoch 302/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 1.6553 - acc: 0.1152 - val_loss: 7.5908 - val_acc: 0.1079\n",
      "Epoch 303/500\n",
      "2518/2518 [==============================] - 0s 86us/step - loss: 1.7034 - acc: 0.1176 - val_loss: 8.3852 - val_acc: 0.1079\n",
      "Epoch 304/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 1.5832 - acc: 0.1168 - val_loss: 8.5936 - val_acc: 0.1063\n",
      "Epoch 305/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.6498 - acc: 0.1164 - val_loss: 7.8904 - val_acc: 0.1079\n",
      "Epoch 306/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 1.5398 - acc: 0.1180 - val_loss: 7.7783 - val_acc: 0.1048\n",
      "Epoch 307/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 1.7547 - acc: 0.1164 - val_loss: 8.6386 - val_acc: 0.1063\n",
      "Epoch 308/500\n",
      "2518/2518 [==============================] - 0s 89us/step - loss: 1.6308 - acc: 0.1176 - val_loss: 9.4094 - val_acc: 0.1048\n",
      "Epoch 309/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.6088 - acc: 0.1156 - val_loss: 9.7961 - val_acc: 0.1048\n",
      "Epoch 310/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 1.8117 - acc: 0.1168 - val_loss: 8.4694 - val_acc: 0.1048\n",
      "Epoch 311/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.6662 - acc: 0.1156 - val_loss: 9.1781 - val_acc: 0.1063\n",
      "Epoch 312/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 1.4602 - acc: 0.1180 - val_loss: 8.5645 - val_acc: 0.1048\n",
      "Epoch 313/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.5430 - acc: 0.1164 - val_loss: 8.7746 - val_acc: 0.1079\n",
      "Epoch 314/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 1.6713 - acc: 0.1164 - val_loss: 9.5512 - val_acc: 0.0905\n",
      "Epoch 315/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.6820 - acc: 0.1160 - val_loss: 8.4094 - val_acc: 0.1048\n",
      "Epoch 316/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 1.6980 - acc: 0.1168 - val_loss: 9.2097 - val_acc: 0.1048\n",
      "Epoch 317/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.5803 - acc: 0.1164 - val_loss: 9.3190 - val_acc: 0.1016\n",
      "Epoch 318/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.5815 - acc: 0.1164 - val_loss: 8.4766 - val_acc: 0.1048\n",
      "Epoch 319/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 1.5768 - acc: 0.1160 - val_loss: 7.7937 - val_acc: 0.1079\n",
      "Epoch 320/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 1.4061 - acc: 0.1164 - val_loss: 8.0917 - val_acc: 0.1079\n",
      "Epoch 321/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 1.6579 - acc: 0.1164 - val_loss: 7.9281 - val_acc: 0.1079\n",
      "Epoch 322/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.5959 - acc: 0.1164 - val_loss: 8.2843 - val_acc: 0.1032\n",
      "Epoch 323/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.4271 - acc: 0.1148 - val_loss: 9.4792 - val_acc: 0.1048\n",
      "Epoch 324/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 1.4676 - acc: 0.1176 - val_loss: 8.2385 - val_acc: 0.1032\n",
      "Epoch 325/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 1.3899 - acc: 0.1172 - val_loss: 8.9611 - val_acc: 0.1048\n",
      "Epoch 326/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 1.4057 - acc: 0.1164 - val_loss: 10.5747 - val_acc: 0.1063\n",
      "Epoch 327/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.4358 - acc: 0.1156 - val_loss: 9.6249 - val_acc: 0.1016\n",
      "Epoch 328/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.6735 - acc: 0.1152 - val_loss: 9.2123 - val_acc: 0.1032\n",
      "Epoch 329/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.4463 - acc: 0.1183 - val_loss: 8.3339 - val_acc: 0.1032\n",
      "Epoch 330/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 1.5232 - acc: 0.1160 - val_loss: 9.0215 - val_acc: 0.1016\n",
      "Epoch 331/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.5200 - acc: 0.1168 - val_loss: 8.9413 - val_acc: 0.1016\n",
      "Epoch 332/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.4295 - acc: 0.1168 - val_loss: 8.4524 - val_acc: 0.1048\n",
      "Epoch 333/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 1.5132 - acc: 0.1164 - val_loss: 9.5089 - val_acc: 0.1048\n",
      "Epoch 334/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 1.3446 - acc: 0.1172 - val_loss: 9.2235 - val_acc: 0.1016\n",
      "Epoch 335/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.4730 - acc: 0.1172 - val_loss: 8.1178 - val_acc: 0.1048\n",
      "Epoch 336/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 1.3456 - acc: 0.1172 - val_loss: 7.7095 - val_acc: 0.1048\n",
      "Epoch 337/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.2599 - acc: 0.1168 - val_loss: 8.6283 - val_acc: 0.1063\n",
      "Epoch 338/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.4591 - acc: 0.1148 - val_loss: 7.5950 - val_acc: 0.1079\n",
      "Epoch 339/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 1.3509 - acc: 0.1168 - val_loss: 8.3778 - val_acc: 0.1063\n",
      "Epoch 340/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.3341 - acc: 0.1180 - val_loss: 8.0130 - val_acc: 0.1016\n",
      "Epoch 341/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.3986 - acc: 0.1172 - val_loss: 8.6753 - val_acc: 0.1063\n",
      "Epoch 342/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.6011 - acc: 0.1148 - val_loss: 9.2907 - val_acc: 0.1032\n",
      "Epoch 343/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 1.3153 - acc: 0.1168 - val_loss: 7.6375 - val_acc: 0.1048\n",
      "Epoch 344/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.4490 - acc: 0.1172 - val_loss: 9.9910 - val_acc: 0.1048\n",
      "Epoch 345/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 1.5529 - acc: 0.1172 - val_loss: 8.2643 - val_acc: 0.1048\n",
      "Epoch 346/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 1.2153 - acc: 0.1168 - val_loss: 7.8030 - val_acc: 0.1063\n",
      "Epoch 347/500\n",
      "2518/2518 [==============================] - 0s 103us/step - loss: 1.4058 - acc: 0.1164 - val_loss: 8.5121 - val_acc: 0.1063\n",
      "Epoch 348/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 1.4134 - acc: 0.1168 - val_loss: 9.0260 - val_acc: 0.1063\n",
      "Epoch 349/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 1.3572 - acc: 0.1168 - val_loss: 8.3951 - val_acc: 0.1032\n",
      "Epoch 350/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 1.6145 - acc: 0.1168 - val_loss: 8.6873 - val_acc: 0.1032\n",
      "Epoch 351/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 1.0582 - acc: 0.1164 - val_loss: 8.8008 - val_acc: 0.1079\n",
      "Epoch 352/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.4451 - acc: 0.1164 - val_loss: 10.0171 - val_acc: 0.1063\n",
      "Epoch 353/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.3364 - acc: 0.1168 - val_loss: 8.4915 - val_acc: 0.1079\n",
      "Epoch 354/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.2290 - acc: 0.1156 - val_loss: 8.1852 - val_acc: 0.1032\n",
      "Epoch 355/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518/2518 [==============================] - 0s 101us/step - loss: 1.3526 - acc: 0.1156 - val_loss: 8.9730 - val_acc: 0.1063\n",
      "Epoch 356/500\n",
      "2518/2518 [==============================] - 0s 90us/step - loss: 1.3688 - acc: 0.1180 - val_loss: 10.1397 - val_acc: 0.1063\n",
      "Epoch 357/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 1.3253 - acc: 0.1160 - val_loss: 8.8516 - val_acc: 0.1063\n",
      "Epoch 358/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 1.4039 - acc: 0.1172 - val_loss: 9.7619 - val_acc: 0.1063\n",
      "Epoch 359/500\n",
      "2518/2518 [==============================] - 0s 107us/step - loss: 1.3974 - acc: 0.1168 - val_loss: 9.7179 - val_acc: 0.1079\n",
      "Epoch 360/500\n",
      "2518/2518 [==============================] - 0s 85us/step - loss: 1.2172 - acc: 0.1168 - val_loss: 8.4985 - val_acc: 0.1063\n",
      "Epoch 361/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.0765 - acc: 0.1164 - val_loss: 11.3748 - val_acc: 0.1063\n",
      "Epoch 362/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.4205 - acc: 0.1160 - val_loss: 9.3107 - val_acc: 0.1095\n",
      "Epoch 363/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 1.4537 - acc: 0.1176 - val_loss: 9.0516 - val_acc: 0.1079\n",
      "Epoch 364/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 1.4959 - acc: 0.1168 - val_loss: 9.6604 - val_acc: 0.1032\n",
      "Epoch 365/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.1587 - acc: 0.1152 - val_loss: 8.1201 - val_acc: 0.1048\n",
      "Epoch 366/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.0533 - acc: 0.1168 - val_loss: 10.0670 - val_acc: 0.1032\n",
      "Epoch 367/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.6020 - acc: 0.1160 - val_loss: 8.4818 - val_acc: 0.1063\n",
      "Epoch 368/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 1.0878 - acc: 0.1180 - val_loss: 9.1789 - val_acc: 0.1048\n",
      "Epoch 369/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.0534 - acc: 0.1172 - val_loss: 8.9881 - val_acc: 0.1063\n",
      "Epoch 370/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 1.2353 - acc: 0.1168 - val_loss: 10.4463 - val_acc: 0.1063\n",
      "Epoch 371/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.1339 - acc: 0.1164 - val_loss: 10.2954 - val_acc: 0.1063\n",
      "Epoch 372/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 1.2781 - acc: 0.1164 - val_loss: 8.7457 - val_acc: 0.1032\n",
      "Epoch 373/500\n",
      "2518/2518 [==============================] - 0s 84us/step - loss: 1.0836 - acc: 0.1168 - val_loss: 9.7302 - val_acc: 0.1032\n",
      "Epoch 374/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 1.0952 - acc: 0.1156 - val_loss: 9.7058 - val_acc: 0.1063\n",
      "Epoch 375/500\n",
      "2518/2518 [==============================] - 0s 104us/step - loss: 1.1466 - acc: 0.1160 - val_loss: 11.1277 - val_acc: 0.1079\n",
      "Epoch 376/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 1.1110 - acc: 0.1176 - val_loss: 9.4992 - val_acc: 0.1095\n",
      "Epoch 377/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.0036 - acc: 0.1187 - val_loss: 9.3268 - val_acc: 0.1063\n",
      "Epoch 378/500\n",
      "2518/2518 [==============================] - 0s 104us/step - loss: 1.1711 - acc: 0.1183 - val_loss: 9.0034 - val_acc: 0.1063\n",
      "Epoch 379/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 1.1905 - acc: 0.1168 - val_loss: 9.4322 - val_acc: 0.1032\n",
      "Epoch 380/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.1137 - acc: 0.1164 - val_loss: 10.1208 - val_acc: 0.1016\n",
      "Epoch 381/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 1.1032 - acc: 0.1180 - val_loss: 8.7569 - val_acc: 0.1063\n",
      "Epoch 382/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.1469 - acc: 0.1168 - val_loss: 10.8410 - val_acc: 0.1048\n",
      "Epoch 383/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 0.9388 - acc: 0.1176 - val_loss: 8.7585 - val_acc: 0.1048\n",
      "Epoch 384/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 0.9616 - acc: 0.1176 - val_loss: 10.2628 - val_acc: 0.1032\n",
      "Epoch 385/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.2259 - acc: 0.1156 - val_loss: 8.2917 - val_acc: 0.1016\n",
      "Epoch 386/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 1.0396 - acc: 0.1160 - val_loss: 8.7952 - val_acc: 0.1032\n",
      "Epoch 387/500\n",
      "2518/2518 [==============================] - 0s 89us/step - loss: 1.2541 - acc: 0.1168 - val_loss: 8.6909 - val_acc: 0.1032\n",
      "Epoch 388/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.0689 - acc: 0.1152 - val_loss: 12.7502 - val_acc: 0.1048\n",
      "Epoch 389/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 1.2028 - acc: 0.1164 - val_loss: 10.1949 - val_acc: 0.1048\n",
      "Epoch 390/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 0.9357 - acc: 0.1187 - val_loss: 9.3147 - val_acc: 0.1063\n",
      "Epoch 391/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 1.0820 - acc: 0.1180 - val_loss: 9.8560 - val_acc: 0.1079\n",
      "Epoch 392/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 1.0358 - acc: 0.1176 - val_loss: 11.4612 - val_acc: 0.1000\n",
      "Epoch 393/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 1.1708 - acc: 0.1180 - val_loss: 9.9861 - val_acc: 0.1063\n",
      "Epoch 394/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.1562 - acc: 0.1176 - val_loss: 9.5194 - val_acc: 0.1032\n",
      "Epoch 395/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.0452 - acc: 0.1160 - val_loss: 9.5034 - val_acc: 0.1063\n",
      "Epoch 396/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.2325 - acc: 0.1176 - val_loss: 9.7320 - val_acc: 0.1048\n",
      "Epoch 397/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 1.1193 - acc: 0.1168 - val_loss: 9.6299 - val_acc: 0.1032\n",
      "Epoch 398/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.0254 - acc: 0.1172 - val_loss: 8.6986 - val_acc: 0.1079\n",
      "Epoch 399/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 1.0380 - acc: 0.1172 - val_loss: 9.4308 - val_acc: 0.1048\n",
      "Epoch 400/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 0.8530 - acc: 0.1168 - val_loss: 9.9325 - val_acc: 0.1032\n",
      "Epoch 401/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 0.9283 - acc: 0.1160 - val_loss: 9.2009 - val_acc: 0.1032\n",
      "Epoch 402/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 1.1721 - acc: 0.1156 - val_loss: 9.1616 - val_acc: 0.1048\n",
      "Epoch 403/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 1.0097 - acc: 0.1172 - val_loss: 10.0338 - val_acc: 0.1000\n",
      "Epoch 404/500\n",
      "2518/2518 [==============================] - ETA: 0s - loss: 1.1409 - acc: 0.111 - 0s 89us/step - loss: 1.0168 - acc: 0.1172 - val_loss: 9.5759 - val_acc: 0.1095\n",
      "Epoch 405/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 1.2320 - acc: 0.1152 - val_loss: 9.1814 - val_acc: 0.1048\n",
      "Epoch 406/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 0.8597 - acc: 0.1172 - val_loss: 9.7682 - val_acc: 0.1063\n",
      "Epoch 407/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.2204 - acc: 0.1168 - val_loss: 8.6398 - val_acc: 0.1048\n",
      "Epoch 408/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 1.0983 - acc: 0.1168 - val_loss: 8.5418 - val_acc: 0.1048\n",
      "Epoch 409/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 0.9673 - acc: 0.1168 - val_loss: 9.1616 - val_acc: 0.1048\n",
      "Epoch 410/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 1.1213 - acc: 0.1168 - val_loss: 10.1620 - val_acc: 0.1032\n",
      "Epoch 411/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 0.9906 - acc: 0.1195 - val_loss: 9.1064 - val_acc: 0.1048\n",
      "Epoch 412/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 0.9884 - acc: 0.1160 - val_loss: 8.8201 - val_acc: 0.1048\n",
      "Epoch 413/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 0.9244 - acc: 0.1160 - val_loss: 7.8256 - val_acc: 0.1063\n",
      "Epoch 414/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.1447 - acc: 0.1176 - val_loss: 8.2903 - val_acc: 0.1063\n",
      "Epoch 415/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 0.9514 - acc: 0.1176 - val_loss: 9.2167 - val_acc: 0.1063\n",
      "Epoch 416/500\n",
      "2518/2518 [==============================] - 0s 85us/step - loss: 0.8148 - acc: 0.1152 - val_loss: 9.3162 - val_acc: 0.1048\n",
      "Epoch 417/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 0.9571 - acc: 0.1180 - val_loss: 9.2204 - val_acc: 0.1063\n",
      "Epoch 418/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 0.7828 - acc: 0.1187 - val_loss: 10.9391 - val_acc: 0.1079\n",
      "Epoch 419/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 0.9373 - acc: 0.1180 - val_loss: 8.7467 - val_acc: 0.1032\n",
      "Epoch 420/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 0.8442 - acc: 0.1191 - val_loss: 8.9379 - val_acc: 0.1048\n",
      "Epoch 421/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 1.0647 - acc: 0.1172 - val_loss: 9.3755 - val_acc: 0.1063\n",
      "Epoch 422/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 0.8826 - acc: 0.1176 - val_loss: 8.8743 - val_acc: 0.1063\n",
      "Epoch 423/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 0.9428 - acc: 0.1183 - val_loss: 8.8683 - val_acc: 0.1048\n",
      "Epoch 424/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.0882 - acc: 0.1176 - val_loss: 7.6732 - val_acc: 0.1079\n",
      "Epoch 425/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 0.9666 - acc: 0.1156 - val_loss: 8.0853 - val_acc: 0.1048\n",
      "Epoch 426/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 0.9476 - acc: 0.1183 - val_loss: 10.8639 - val_acc: 0.1063\n",
      "Epoch 427/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 0.9902 - acc: 0.1168 - val_loss: 8.5291 - val_acc: 0.1063\n",
      "Epoch 428/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 1.0504 - acc: 0.1180 - val_loss: 8.0924 - val_acc: 0.1063\n",
      "Epoch 429/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 0.7630 - acc: 0.1183 - val_loss: 8.4543 - val_acc: 0.1063\n",
      "Epoch 430/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 0.9272 - acc: 0.1180 - val_loss: 9.9100 - val_acc: 0.1063\n",
      "Epoch 431/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 0.9000 - acc: 0.1172 - val_loss: 7.6416 - val_acc: 0.1063\n",
      "Epoch 432/500\n",
      "2518/2518 [==============================] - 0s 92us/step - loss: 0.9569 - acc: 0.1164 - val_loss: 10.5973 - val_acc: 0.1079\n",
      "Epoch 433/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 1.2103 - acc: 0.1160 - val_loss: 8.6328 - val_acc: 0.1048\n",
      "Epoch 434/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 0.9092 - acc: 0.1176 - val_loss: 9.5536 - val_acc: 0.1063\n",
      "Epoch 435/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 0.7888 - acc: 0.1183 - val_loss: 9.3150 - val_acc: 0.1063\n",
      "Epoch 436/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 1.1092 - acc: 0.1164 - val_loss: 9.5214 - val_acc: 0.1079\n",
      "Epoch 437/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 1.1269 - acc: 0.1176 - val_loss: 9.5733 - val_acc: 0.1063\n",
      "Epoch 438/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 0.7178 - acc: 0.1180 - val_loss: 9.7222 - val_acc: 0.1079\n",
      "Epoch 439/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 0.8310 - acc: 0.1164 - val_loss: 11.3329 - val_acc: 0.1032\n",
      "Epoch 440/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 1.1069 - acc: 0.1164 - val_loss: 10.8425 - val_acc: 0.1032\n",
      "Epoch 441/500\n",
      "2518/2518 [==============================] - 0s 104us/step - loss: 0.7936 - acc: 0.1176 - val_loss: 8.7991 - val_acc: 0.1048\n",
      "Epoch 442/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 0.9852 - acc: 0.1176 - val_loss: 9.5066 - val_acc: 0.1063\n",
      "Epoch 443/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 1.1224 - acc: 0.1172 - val_loss: 7.9144 - val_acc: 0.1063\n",
      "Epoch 444/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 0.9753 - acc: 0.1180 - val_loss: 9.3696 - val_acc: 0.1063\n",
      "Epoch 445/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 1.0009 - acc: 0.1176 - val_loss: 9.2452 - val_acc: 0.1063\n",
      "Epoch 446/500\n",
      "2518/2518 [==============================] - 0s 87us/step - loss: 0.9582 - acc: 0.1168 - val_loss: 9.5059 - val_acc: 0.1063\n",
      "Epoch 447/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 0.7380 - acc: 0.1183 - val_loss: 8.9567 - val_acc: 0.1063\n",
      "Epoch 448/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 0.7848 - acc: 0.1168 - val_loss: 10.8377 - val_acc: 0.1032\n",
      "Epoch 449/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 1.1070 - acc: 0.1180 - val_loss: 9.7539 - val_acc: 0.1079\n",
      "Epoch 450/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 0.8362 - acc: 0.1183 - val_loss: 9.2228 - val_acc: 0.1032\n",
      "Epoch 451/500\n",
      "2518/2518 [==============================] - 0s 94us/step - loss: 0.8825 - acc: 0.1187 - val_loss: 9.5379 - val_acc: 0.1079\n",
      "Epoch 452/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 0.9539 - acc: 0.1168 - val_loss: 7.7592 - val_acc: 0.1079\n",
      "Epoch 453/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 0.9134 - acc: 0.1191 - val_loss: 8.6425 - val_acc: 0.1079\n",
      "Epoch 454/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 0.9659 - acc: 0.1172 - val_loss: 8.7071 - val_acc: 0.1048\n",
      "Epoch 455/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 0.8367 - acc: 0.1160 - val_loss: 9.6249 - val_acc: 0.1063\n",
      "Epoch 456/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 0.8779 - acc: 0.1187 - val_loss: 10.5915 - val_acc: 0.1063\n",
      "Epoch 457/500\n",
      "2518/2518 [==============================] - 0s 88us/step - loss: 0.8488 - acc: 0.1168 - val_loss: 8.6839 - val_acc: 0.1063\n",
      "Epoch 458/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 0.9650 - acc: 0.1160 - val_loss: 8.4687 - val_acc: 0.1048\n",
      "Epoch 459/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 0.7776 - acc: 0.1172 - val_loss: 9.1877 - val_acc: 0.1063\n",
      "Epoch 460/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 0.8095 - acc: 0.1172 - val_loss: 9.6400 - val_acc: 0.1063\n",
      "Epoch 461/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 1.0182 - acc: 0.1176 - val_loss: 10.6304 - val_acc: 0.1048\n",
      "Epoch 462/500\n",
      "2518/2518 [==============================] - 0s 101us/step - loss: 0.8562 - acc: 0.1187 - val_loss: 9.0063 - val_acc: 0.1032\n",
      "Epoch 463/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 1.0245 - acc: 0.1172 - val_loss: 9.0343 - val_acc: 0.1032\n",
      "Epoch 464/500\n",
      "2518/2518 [==============================] - 0s 103us/step - loss: 0.7903 - acc: 0.1176 - val_loss: 10.6022 - val_acc: 0.1063\n",
      "Epoch 465/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 0.7009 - acc: 0.1183 - val_loss: 9.8555 - val_acc: 0.1063\n",
      "Epoch 466/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 0.6953 - acc: 0.1180 - val_loss: 8.0495 - val_acc: 0.1063\n",
      "Epoch 467/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 0.9478 - acc: 0.1176 - val_loss: 9.4907 - val_acc: 0.1063\n",
      "Epoch 468/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 0.6157 - acc: 0.1183 - val_loss: 9.0831 - val_acc: 0.1063\n",
      "Epoch 469/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 0.8740 - acc: 0.1172 - val_loss: 11.6771 - val_acc: 0.1048\n",
      "Epoch 470/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 0.8050 - acc: 0.1187 - val_loss: 9.8500 - val_acc: 0.1063\n",
      "Epoch 471/500\n",
      "2518/2518 [==============================] - 0s 103us/step - loss: 0.9969 - acc: 0.1172 - val_loss: 8.8322 - val_acc: 0.1079\n",
      "Epoch 472/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 0.7828 - acc: 0.1187 - val_loss: 9.0537 - val_acc: 0.1063\n",
      "Epoch 473/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518/2518 [==============================] - 0s 99us/step - loss: 0.9598 - acc: 0.1172 - val_loss: 8.9631 - val_acc: 0.1063\n",
      "Epoch 474/500\n",
      "2518/2518 [==============================] - 0s 105us/step - loss: 0.7641 - acc: 0.1176 - val_loss: 8.6088 - val_acc: 0.1048\n",
      "Epoch 475/500\n",
      "2518/2518 [==============================] - 0s 89us/step - loss: 0.8973 - acc: 0.1180 - val_loss: 9.7442 - val_acc: 0.1048\n",
      "Epoch 476/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 0.7529 - acc: 0.1180 - val_loss: 10.5431 - val_acc: 0.1063\n",
      "Epoch 477/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 0.9684 - acc: 0.1187 - val_loss: 8.6550 - val_acc: 0.1032\n",
      "Epoch 478/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 0.6770 - acc: 0.1183 - val_loss: 10.1521 - val_acc: 0.1063\n",
      "Epoch 479/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 0.7725 - acc: 0.1176 - val_loss: 9.7032 - val_acc: 0.1079\n",
      "Epoch 480/500\n",
      "2518/2518 [==============================] - 0s 97us/step - loss: 0.8021 - acc: 0.1187 - val_loss: 9.8304 - val_acc: 0.1048\n",
      "Epoch 481/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 0.7890 - acc: 0.1176 - val_loss: 8.4076 - val_acc: 0.1048\n",
      "Epoch 482/500\n",
      "2518/2518 [==============================] - 0s 98us/step - loss: 1.0170 - acc: 0.1172 - val_loss: 9.5532 - val_acc: 0.1048\n",
      "Epoch 483/500\n",
      "2518/2518 [==============================] - 0s 104us/step - loss: 0.6452 - acc: 0.1160 - val_loss: 9.3291 - val_acc: 0.1063\n",
      "Epoch 484/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 0.7782 - acc: 0.1180 - val_loss: 9.2659 - val_acc: 0.1079\n",
      "Epoch 485/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 0.6967 - acc: 0.1180 - val_loss: 9.7658 - val_acc: 0.1063\n",
      "Epoch 486/500\n",
      "2518/2518 [==============================] - 0s 103us/step - loss: 0.9250 - acc: 0.1180 - val_loss: 9.4945 - val_acc: 0.1032\n",
      "Epoch 487/500\n",
      "2518/2518 [==============================] - 0s 91us/step - loss: 0.6421 - acc: 0.1180 - val_loss: 10.0017 - val_acc: 0.1063\n",
      "Epoch 488/500\n",
      "2518/2518 [==============================] - 0s 93us/step - loss: 0.6804 - acc: 0.1183 - val_loss: 10.7446 - val_acc: 0.1048\n",
      "Epoch 489/500\n",
      "2518/2518 [==============================] - 0s 96us/step - loss: 0.7696 - acc: 0.1176 - val_loss: 9.2847 - val_acc: 0.1032\n",
      "Epoch 490/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 0.7181 - acc: 0.1172 - val_loss: 8.2421 - val_acc: 0.1063\n",
      "Epoch 491/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 0.8711 - acc: 0.1168 - val_loss: 8.7587 - val_acc: 0.1063\n",
      "Epoch 492/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 0.7226 - acc: 0.1156 - val_loss: 10.6393 - val_acc: 0.1048\n",
      "Epoch 493/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 0.8644 - acc: 0.1180 - val_loss: 10.1054 - val_acc: 0.1063\n",
      "Epoch 494/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 0.8781 - acc: 0.1180 - val_loss: 9.5904 - val_acc: 0.1063\n",
      "Epoch 495/500\n",
      "2518/2518 [==============================] - 0s 102us/step - loss: 0.7378 - acc: 0.1187 - val_loss: 8.2582 - val_acc: 0.1079\n",
      "Epoch 496/500\n",
      "2518/2518 [==============================] - 0s 95us/step - loss: 0.8144 - acc: 0.1164 - val_loss: 9.3174 - val_acc: 0.1063\n",
      "Epoch 497/500\n",
      "2518/2518 [==============================] - 0s 105us/step - loss: 0.7550 - acc: 0.1164 - val_loss: 12.0676 - val_acc: 0.1063\n",
      "Epoch 498/500\n",
      "2518/2518 [==============================] - 0s 99us/step - loss: 0.6602 - acc: 0.1183 - val_loss: 10.5279 - val_acc: 0.1048\n",
      "Epoch 499/500\n",
      "2518/2518 [==============================] - 0s 89us/step - loss: 0.7497 - acc: 0.1164 - val_loss: 9.9379 - val_acc: 0.1079\n",
      "Epoch 500/500\n",
      "2518/2518 [==============================] - 0s 100us/step - loss: 0.8992 - acc: 0.1168 - val_loss: 9.7043 - val_acc: 0.1016\n"
     ]
    }
   ],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs):\n",
    "    if epoch % 100 == 0: print('')\n",
    "    print('.', end='')\n",
    "\n",
    "EPOCHS = 500\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "# Store training stats\n",
    "#history = model.fit(X_train, y_train, epochs=EPOCHS,                    validation_split=0.2, verbose=1,                    callbacks=[early_stop]                    )\n",
    "history = model.fit(X_train, y_train, \n",
    "                        epochs=EPOCHS,                \n",
    "                        validation_split=0.2, verbose=1       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Abs Error ')\n",
    "  plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),\n",
    "           label='Train Loss')\n",
    "  plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n",
    "           label = 'Val loss')\n",
    "  plt.legend()\n",
    "  plt.ylim([0, 10])\n",
    "\n",
    "#plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.826033039799443, 0.09555555555555556]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.43904,  0.53756,  0.     , 23.01359, 14.95285, 20.03666,\n",
       "        1.92956,  0.     , 23.2391 , 19.37443,  0.86473, 19.71981,\n",
       "       22.14493,  5.14496, 19.83196, 11.68536,  3.74549,  5.01165,\n",
       "       21.88536, 14.24529, 24.78625,  4.00666, 21.84183, 14.25459,\n",
       "       12.91725, 12.08076, 13.18412, 14.10838,  2.76624, 23.11559,\n",
       "        1.0778 ,  9.07721, 21.06963,  6.52759,  1.11933,  0.51372,\n",
       "        4.66541,  0.     ,  4.16056,  0.     ,  7.90326, 23.39682,\n",
       "       12.36187,  1.27219,  0.     , 11.04429,  5.90135, 23.89068,\n",
       "       21.30902])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.430144152785048"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.sum(np.subtract(model.predict(X_test),y_test))/np.sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set Mean Abs Error: $   0.10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "[loss, mae] = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Testing set Mean Abs Error: ${:7.2f}\".format(mae)) #1.47\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xt8XPV55/HPI3lsy9hGEAQLKo5dh+JCSe1GpW7NqwGaAknaxHUv5EKT7vKq6TZpSMt6MdluQ9oEnJAL2UvIQp0WGkqA4igkZjH3UAiEyJGNIcaGAKYIr+0UBAIJW5dn/5gz8mh0zsyZ0Zy5ne/79fLL0mguP49H5znn+T2/52fujoiIpFdbvQcgIiL1pUAgIpJyCgQiIimnQCAiknIKBCIiKadAICKScgoEIiIpp0AgIpJyCgQiIik3q94DiOOYY47xxYsX13sYItKk3hyd4LmfvQ7Az3fNZ86sdJwDb9269Wfu3lXqfk0RCBYvXkxfX1+9hyEiTWj3viE+dN2jnGrGTWtXsrRrfr2HVDNmtifO/dIRFkUklXJBoC2FQaAciQUCMzvRzO43s51m9qSZXRzcfrmZDZjZtuDPe5Iag4ikl4JAfEmmhsaAS9z9x2a2ANhqZncHP/uKu38xwdcWkRRTEChPYoHA3fcCe4Ovh8xsJ9Cd1OuJiICCQCVqMkdgZouBFcAPg5s+bmaPm9k3zOyoWoxBRFqfgkBlEg8EZjYfuA34pLu/BlwDLAWWk71i+FLE49aaWZ+Z9R04cCDpYYpIE+vtH+D0z93DOV95kJffOMSFZyxREChDooHAzDJkg8CN7r4JwN33ufu4u08A1wGnhz3W3a919x537+nqKlkGKyIp1ds/wKW3Pc7+oYMATDhcfc/T9PYP1HlkzSPJqiEDNgI73f3Lebcfn3e33wOeSGoMItL6rrhjJwfHJqbcNjI6zlVbdtVpRM0nyaqhVcAfAzvMbFtw26eAD5rZcsCB54GLEhyDiLSw3fuGJq8ECr00OFLj0TSvJKuGHgIs5Ed3JPWaIpIehyeGs+mgQid0dtR+UE2qKVpMiIjku+aBZ7hqyy4mHBbMmcWbY+OMjh+OBh2Zdtade3IdR9hcFAhEqqy3f4CrtuzipcERTujsYN25J7N6hZbQVMs1DzzD5+88nP8fOjhGps04al6GweFRvecVUCAQqaLe/gEu27SDkdFxAAYGR7hs0w4AHZiKiBs8d+8bCp0EHp1w5s2eRf/fnFOL4bYcNZ0TqaKrtuyaDAI5qmAprrd/gHW3bmdgcAQnGzzX3bp9Wvlnbk4gbD4ANDk8E7oiEKmiqIORDlLRLr/9SUYLju6jE86nNj0+eZXQtWAOw4fGmTe7nWMXzAmtFNLkcOV0RSBSRVEHIx2kog2OjIbePjw6MXmVsH/oIK8fHOPCM5bwqff8Ih2Z9in31eTwzCgQiFTRunNP1kEqQTc8sofVK7q5cs1pdHd2YEB3ZwdXrjlNczAzoNSQSBXlDkaqGorvqHkZXhkOvyooNBCk2Fav6NZ7WkUKBCJVpoNUeT79u6dyya3bGY+aBc7TbmFrVGWmFAhEpO7agPGS94Jxd3r7B/jMd5+cvIqYl2lj9qx2Xh3RGoJKKRCISF1dtWXXtKqhYj5587Yp3w+PTjA8mm06p3UbldFksYjUVbVLa7Vuo3wKBCJSV1GltZ0dGborLLvVuo3yKBCISF1dsHLRtNs6Mu1c/r5TeXj92aEtjEvRuo3yKBCISN3kuojC4YNR4bqAcg/qWrdRPgUCEamLXBfR3Dxxbo+x4UNjU+4XtkgvihaXVUZVQyKSiGIdRaO6iAK8Mjw6pfKncJFemxnjPr3KqLuzg4fXn53Qv6a1KRCISCzl7LNQrB33KScsLNpFFA5X/uSePz8gFD43KB00UwoEIlJS2IH9L2/eRt+el/ns6tOm3T+qHfcVd+xkwp02M9o4nA4KE1X5ozYe1adAICIlhR3YHbjx0RfoeevR0w7CUQfx/UMHOXbBHG5au5Lf+tL3i77mCZ0dkVchauNRXZosFpGSog7sDqG5/mKVPvuHDvKRjY9x1LxM5H06Mu2ctayLyzbtmLJhzWWbdkzbsEZmToFAREoqdmAPCxKlKn0GBkd4/c0xMu3TVwkcNS/DlWtO4/6nDmi3txpRIBCRktade3Lkwi4HVm24b8qZ+uoV3fz+O4qnbkYnnLEJp7MjM7mvwNXnL6f/b85h9Ypu7fZWQwoEIlJULk9frC1cYdqmt3+AW/teLPnc7vDGoTGO7Mjw0uAIV23ZNfkc2u2tdhQIRCRSrlpoIO8sPOrKID9tc8UdOzk4Vqwm6LDRcWdwZHRyHuCTN29jxd/exeK3hB/wz1rWVc4/QWJQIBCRSFHVQlFeGhxh976h0M3ly/HK8Cg/+OnLoT+7/6kDM3pumU7lo02knAU9IpUo/IwNlJmP71owhzVf+0FVxhIVcDRHUH0KBE2i2EpNBQOphrDPmBF+QD5qXoY3RyemXC3MmdXGa2+O8uZovJRQpTRHUH1KDTWJqJWaKqWTaolKAxXOCXRk2vn0757KlWtOo7uzAwOOXTCHTHsbhyLmBdrNiq4biEutJJKhK4ImoVI6SVqxRWPdnR2hKcnVK7rZvW+ID133KPNmt/P6wbHQ55hwZzDYY7hcuauSbqVDE6NA0CSi8rW6TJZqifqMFXb17O0fYNWG+3hpcISuBXMYPjTOvNnt3LR2JR/Z+FjRz2m5cw5wOAios2hyEksNmdmJZna/me00syfN7OLg9qPN7G4zezr4+6ikxtBKwlZq6jJZqinOZyy/nNTJtot4/eAYF56xhKVd84s+x7pzTybTVsl+Y7ryTVqScwRjwCXu/ovASuBjZnYKsB64191PAu4NvpcSVq/onpKT1QYcUm25z1h+Ln/OrKmHiLB5BIAbHtkz5TnCPqerV3Qzf25lSYhiV765K5Ql6zdPW+Es8SSWGnL3vcDe4OshM9sJdAPvB84M7nY98ABwaVLjaCXquCi1kF/1MzgydZOYOHNVxT6nxeYJDDiyI8Mbh8YYHT9cq1TsylfVdNVRk6ohM1sMrAB+CBwXBIlcsDi2FmMQkdJKVad1LZgT+ri4c1VR92u3bMroiDmzOP9XT4x95atquupIfLLYzOYDtwGfdPfXzOLlCM1sLbAWYNGiRckNUKTFFFt4mPvZwOAI7cGWj/nVOFGTubkVw8OHpqeFypmrOmtZFzc++sK0tQm5rScHBke4betA7LSnqumqI9FAYGYZskHgRnffFNy8z8yOd/e9ZnY8sD/sse5+LXAtQE9PT7FV7SISKJYqAab8LP/ge9mmHdza90Lk83YtmDNZIvoXZ7+NGx7ZU/YK997+AW7bOlC0RQVM36ayGFXTVUdigcCyp/4bgZ3u/uW8H90OfBTYEPz9naTGIJI2pVIlYRO9udsfjujtA0wpEV3aNZ+L3rm0KmOLEveMft25J2v/4ipI8opgFfDHwA4z2xbc9imyAeAWM7sQeAH4wwTHIJIqSaVK8oNAmDh9sMoZQ9wzeu1fXB1JVg09RHTH2t9K6nVF0qxUqqSSBV1tRskgEKdyJ2pshf2Myj2jVzXdzKnXkEgLKbWgK2r7yGIlHO99+/HTgkB+7f4lt2yPVbkTNbYPr1yk9TF1phYTIi0kTqokrGrorGVd3LZ1IDSH/93te/nX3Xdx+ftOZfWK7mlXALlJ50KFqSClcRqXecR/YiPp6enxvr6+eg9DpCYq3XdipvtV9PYP8Fc3byOqiXRHpp3ff0c3N/3w3yIP/vnUH6j+zGyru/eUup+uCEQaSKUrZauxwvaUExZGBgHIpnvC1gCEUeVOc9EcgUgDqXSl7ExX2OZaSZfqCVcsCLSbKc/fpHRFINJAKi3/nEnZ6OEgYFx63jKuvufp2PX+OR2Zdh38m5gCgUhCKsnZV7pStnNehldCGrqVelx+EMiViB63cC6f+e6T054vatvKdrOSQUD7bTc2BYKU0y9oMirN2ZezUja/b1CYTLsVzdOHBYHc+HLVQfmfjbDKojhXAuoQ2vgUCFJMv6DJKZazL/bexi2xLPy/C3PE7FmRrxUWBMJOCgqrfnreenTZJw6VvhdSOwoEKaZf0OTMJGcfZ6VsnL49r46E9/6PCgJxTgoqWcWrDqGNT1VDKaZf0ORE5ear0RWzt38gVquIsNeKSgcl2dc/yfdCqkOBIMX0C5qcpPaYzp25lxL2WlFBAKJ7EFXjpCDuXsjabrJ+FAhSLKmDlSS3x3SxlFBuCUDYaxULAr39A5G9hqpxUlDqvcgFt4HBEZzDaSkFg9rRHEGKqfdLspLoilnsDP3DKxdx/1MHeGlwZDKls3pFd9EgANn//6iFYmct66rKuIu9F5qrqj8FgpRTC9/mErXOoLMjM6W0M3dWvffVETY+9BxtZlx4xhI+svGxaUG/WHC5/6kDif1bcjRXVX9KDYk0kah0ntn03cdyZ9W5IHD1PU+Hpl+KpX9qcTDWXFX9KRCINJGofPtgyKpigAnPbipzwyN7IgNFsTmhIzsyscdW6YSv5qrqT6khkQZUbMV3WDovrCUEwMK5s1jaNb9o+mX1im7+27d38Mah6ZPQVqIJXf54K12cqLmq+lMgEGkwlRxUo7YHaAuO5KV6GA2HBAEg8kqj0EwnfDVXVV9KDYk0mEoWd0WtIs7dXir9MtM8vSZ8m5sCgbSkZl6gVMlBtWvBnNDbcwfyUrX8M83Ta8K3uSk1JC2n2ZvplduKeve+odDUTuGBvFj6ZaZ5+nK6pkrjUSBoImoZHU8jLFCayf9VqYNq/nN3LZjD8KFx5s1u5y/Ofhs3PLKn4s/HTPL0mvBtbtq8vkmEtR3WrlDhlqzfHLpS1oDnNrw38dcP+7/KberSXeIAmb/HQLsZ4+5THhPVfnrh3FkMvTmmA7BMoc3rW0wjnOU2i0p3+aqWsP+rXGAKS1PlH/zzdwEbd5+8Esg/4w7rNfTam2ORzx9GV5eST5PFTUJVGfHVe4FSqf+T/Aqg/IZrMH0ryMJqoTj/36UqjNTkTQopEDSJqBWe5az8TIukOn/GFefKI3dAj7PBTP7Bf/6ceBfxxQLG5bc/GXp1eckt2xUMUkqpoSYRtcIz7srPtKnnAqWwyd5CuWAR5ww/d9/d+4Z4/eBYrDE4sGrDfdNSPn/du4PBiDUH4+6haSWlkVqfrgiaRNQKz7grP6V28q9IgGm9/uMs5Cq8b66VdDmlHYUpn97+AW589IWijylMKymNlA4KBE1CC3aay+oV3Ty8/mye3/BevnL+8rIWchVuMLP31RHOu/pBfvb6ochf2PaIS8P8A3uxfQfy5V+lJLmFpTQOpYaahBbsNK+ZLOS65oFn+Pydhw+6EyHPkSsj/subt4Ue6HMH9riFBZ3zDs87qUghHRILBGb2DeB3gP3u/kvBbZcDfwrkdrv4lLvfkdQYWokW7LSuqECxe99Q5Jl3uxkT7lM+B7kS1EK5q8aostpC+UuL6l2KK7URKxCY2cXAPwBDwN8DK4D17n5XkYf9I/C/gBsKbv+Ku3+x/KGKOjSmR25OYCIilzPhPm1xXKmrxjiT2DC1gZ2uRNMh7hXBf3L3r5rZuUAX8B/JBobIQODuD5rZ4hmPUCapeiMd8vcYPnbBHPYPHZx2n7Az8lJXjYU/bwtWLhd77iSuRPU5bjxxA0FuJuo9wD+4+3azigsXP25mHwH6gEvc/ZUKnydVmr2RmsRTuNH8jhdfLeuMvNRVY/7Po9qWFD53Na9E9TluTHGrhraa2V1kA8EWM1tA+LxVKdcAS4HlwF7gS1F3NLO1ZtZnZn0HDiS/gXajU/VG6ysMAku75k+WonbmLRycm6lOsV89Ft7pc9yY4l4RXEj24P2suw+b2VvIpofK4u77cl+b2XXA94rc91rgWsg2nSv3tVqNqjdaW1gQyHdw7PB51yvDo1U7i671vJM+x40p1qmFu08A+4BTzOw3gVOBznJfzMyOz/v294Anyn2OtNI6gsZR7U1vSgWBVjqL1ue4McWtGvo8cD7wEyD3iXTgwSKPuQk4EzjGzF4EPg2caWbLg8c+D1xU6cDTRtUbjSFujjvuhGipIJB7jTBxSkEbjT7HjSluamg1cLK7Ty9fiODuHwy5eWPcx8tUWkfQGOK0A48bLOIEAWByX4Kw25uNPseNKW4geBbIALEDgVSf1hHUX5wcd5xgERYEoq4iwoIAEHl7o9PnuPHEDQTDwDYzu5e8YODun0hkVCINKmqlbZsZS9ZvLrp6Nxcs8oPAhWcs4SMbH5u2KU3+VUR3xHN2K68uVRK3Du124O+AHwBb8/6IpEpYkzjInp3nunNGJWxO6OyYFgSuvufpkpvS1HujHWl9sa4I3P16M5sN/EJw0y53V/9jSZ04q3MdppzdQ/bAfcHKRVPSQR/Z+FisTWnC8upnLeviqi27+MubtynPLjMWt2roTOB6spU+BpxoZh9198iqIZFWlZ/jXrJ+c+h9chvV5w7cF6xcxMaHnpsyJ1DOpjTFVgRrda7MVNw5gi8B57j7LgAz+wXgJuAdSQ1MpBkc2ZEJ3fGrsyPDw+vPBqKrg0p1A41K/8SZjBYpR9w5gkwuCAC4+26yVUQiqVZqC9FiJaLrzj05cj6h3Syy3YNW50q1xb0i6DOzjcA/Bd9/GE0Wywy1QhfKYluIllonsHpFN317XubGR1+YNp9QrOeP9giQajOPUYtsZnOAjwFnkJ0jeBD4WjkLzGaip6fH+/r6avFSUiNRnS+TbnpWbas23Bd6UD52wRwm3EsuFoPyA2Kx9w60WEsOM7Ot7t5T8n5xAkG9KRC0nqgDaHdnx2RuvRmEHZTnzGoj097GvNntJYPATF638IAPtERwleqJGwiKpobM7BZ3/yMz28H0Mmfc/e0zGKOkWKvkuQtLO7sWzGH40HiiQSD3uoUH91Ub7mvqSeRWSBU2q1JzBBcHf/9O0gORdGmlPHfuoJybE0g6CERp5uCqktj6Klo15O57gy//3N335P8B/jz54UmrarXVsnEbyCWpmVs8t1Kr7WYUt3z0t0Nue3c1ByLpUo/dsZLSCEEAmju4NvPVTCsoNUfwn8me+S81s8fzfrSAbN8hkYoV60LZLPniRgkC0NwtnlspVdiMSs0R/DPwf4ErgfV5tw+5+8uJjUpSrVnyxY0UBHKatcWzNqypr1JzBK+6+/PAV4GX8+YHRs3s12oxQEmfZsgXN2IQaGatlCpsRnFXFl8D/Ere92+E3CZSFY2eL65lEGiWFFk1NOvVTCuIGwjM81aeufuEmcV9rEhsvf0Doa2doTHyxbUOAs2QIpPmF7dq6Fkz+4SZZYI/F5PdvlKkanIHvrAg0Aj54lqng5ohRSatIW4g+DPgN4AB4EXg14C1SQ1K0inswAfFO3HWSj3mBBo9RSatI+4OZfuBDyQ8Fkm5qAPchHvqggCopFJqp9Q6gv/q7l8ws/9JeK8hbV4vVVPqwFePidOkg0Cxf5NKKqVWSl0R7Az+VutPSVyxA189Jk5rEQSK/ZuaeYGYNBe1oZaGEnWGXOu21dUOAmH/rqu27GqJVtzSuKrVhvq7hKSEctz9fRWMTSRSVC15rSZOe/sHuOKOnewfOkibwaXnLatKEAg78w+bGAdNBkvtlUoNfTH4ew3wH4BvBt9/EHg+oTGJTFOLidPe/gEuve1xDo5NADDhcPU9T3PcwrkzSsdElYG2N/B6CUmXUi0mvu/u3wdWuPv57v7d4M+HyG5bKVITteisecUdOyeDQE416vajzvDH3Zu2W6i0lrjrCLrM7Odz35jZEqArmSGJTJd0L5rd+4bYPxS+BfdMUjW5ldJhcv8G9deReou7ef15wLUcXk28GLjI3bckN7TDNFksScpNDL/8xiEmIn4duiuo2AnbzzhHewlLLVRlsjjH3e80s5OAZcFNT7l7+OmTSBPJrw669LxlXH3P06EH7rBy1VLrGhp5pbRIvlipITObB6wDPu7u24FFZlZ0H2Mz+4aZ7TezJ/JuO9rM7jazp4O/j5rR6CWVevsHWLXhPpas38yqDffR2z9Q0fMUlohe9M6lk6maMPnzBbmz/YHBEZzDgSJ/LI26UlqkUNw5gn8ADgG/Hnz/IvDZEo/5R+C8gtvWA/e6+0nAvUzd7EYkVP6Bf/ln7mLdv2wvegCOI2qdwOoV3Ty8/mzCs/qHD+5xGsI18x7Cki5xA8FSd/8CMArg7iMQ+btCcJ8HgcJdzN4PXB98fT2wOv5QJY0Kz7wHR0YZHZ+ayC+3sifOYrFSB/E46xqaeQ9hSZe4geCQmXUQLC4zs6VAJXMEx7n7XoDg72MreA5pUpWkdKLy7IXiVvbEXTEcdhDPtBnDh8ZYsn5zZCVQfgDRrlvSLOJuLvNp4E7gRDO7EVgF/ElSgwIws7UEra4XLVqU5EtJDVTaKyjuAT5OuqWcthGFfX6O7MjwxqExXhkeBYi9Z4J23ZJmUDIQmJkBT5FdXbySbEroYnf/WQWvt8/Mjnf3vWZ2PLA/6o7ufi3ZklV6enoavyGSFFUsp17sQBm1ojhfnHRLJb2D8g/iqzbcx+DI6LT7tJsx4R5aNZSmbSaluZUMBO7uZtbr7u8ANs/w9W4HPgpsCP7+zgyfT5pEpb2CwjqSZtqM+XNnMTg8GusAW40GcsUqgJ7b8N5pt2ubSWkmcVNDj5rZr7r7j+I+sZndBJwJHGNmL5JNL20AbjGzC4EXgD8sc7zSpCrtFTTTVszV6iJa7vgrvQISqYe4geAs4M/M7HngDbLpIXf3t0c9wN0/GPGj3yprhNISZrLJSqV59mq2ki53/NpmUppJ3EDw7kRHIS2v1pusVHs/gXLHr20mpZkU7TVkZnPJblz/NmAHsNHdx2o0tknqNSTlCAsCYRO3kFxgCusz1Ir9hSqZENckeu1Uq9fQ9WQXkf0r2auCU4CLZz48kWREBYHCidt1t24HY3JxWrUnc9OwzWQlE+KaRG9MpQLBKe5+GoCZbQQeS35IIqUVnlWetayLu57cF7qzWNjE7WhIm9FqT+a2+hqCSibENYnemEoFgsnCaXcfs4jVlCK1FHZW+c1HX5j8+YTDF7fs4uvf/ymDw6PRe62G0GRufJVMiGsSvTGVajHxy2b2WvBnCHh77msze60WAxQpFKftxOiE80qZQQA0mVuOSprqqRFfYyq1VWW7uy8M/ixw91l5Xy+s1SBF8lXj7DHTZmTap17hqiFceSppqqdGfI0pbvmoSMOI03YiigWPT7pqKA0qmRBPwyR6M4q1VWW9qXw0naLKDHv7B7j0tsenbTRfSndnBw+vPzuh0TYulWumV1W3qhSptWJlhqecsJBMexuj4xNMePYAf9ayLu5/6sCUTqH5+xakNf2gck2JQ4FAGlJUmeEVd+xkwp15s9v5zsdXRa4Y1llwlso1JQ4FAmlIURPC+4cOcuyCOaFtI3Twn07lmhJH3B3KRGoqqpywzYgMAqU2k08jlWtKHAoE0pDCygxh6orhfHE2k08jlWtKHEoNSUPKpXSuuGPnlLYRF71zaej9lQIJp3JNiUOBQBpCWH7/lBMWMuEeOSeQT22fo7V6zyOZOQUCqbuwEsdLb3ucTHsb82a3x9pPYCYb34iknQKB1F1Yfv/g2ASj4xNFS0TzKQUiUjkFAqm76I3hKWtnMaVARCqjqiGpu6g8frfy+yI1oUAgdbfu3JOZM2vqR1H5fZHaUWpI6i6sd5Dy+yK1o0AgiYjb7iG3x3Bh76De/gFWbbhPE78iNaBAIFUXt+Nl2Ebz5TxeRKpDcwRSdXHaPeSCwKFgT4F3fen7rNpw3+SVhNpFiNSOrgik6kq1e7jmgWf4wp27JvcTfu3NMeDwmX/UfsRpbxchkhRdEUjVFet4ec0Dz/D5vCBQaGR0nHaz0J+pXYRIMhQIpOqiOl5esHJRrPTOuDuFoUDlpCLJUWpIqr6hS1i7hwtWLmLjQ88xEXOLbCe70byjclKRpCkQpFxSFTr57R7yq4MWzp01OSdQSi4IFG44r53IRKpLqaGUS7pCJz8IXHjGEkYOhU8ERymcINZOZCLVp0CQcjPd0CW38GvJ+s2T5Z85hesEbnhkD6MhuSEDOjsyoc9fOEGs0lKR6qtLIDCz581sh5ltM7O+eoxBsmayp22xs/OwxWLFgsvl7zs11paK2olMpPrqOUdwlrv/rI6vL8xsQ5eos/Mr7tjJhPu0FcPFdhGLu5+AdiITqT5NFqdcJRu65CZrww7IAPuHDoZuL1kq6MTZT0A7kbU+FQPUnrnHrOer5ouaPQe8QrYw5P+4+7Uh91kLrAVYtGjRO/bs2VPbQTaxJH+RCquMwrQZ3P1X7wzdVKYaY9OBonWFfb46Mu1cueY0/R9XwMy2untPyfvVKRCc4O4vmdmxwN3AX7j7g1H37+np8b4+TSXEkfQv0qoN90VeCeRc9u5lXPTOpTN+LUmfqM9XWBmxlBY3ENQlNeTuLwV/7zezbwOnA5GBQOIrVlVTjTPvUpOyswyOWzi37HGLgIoB6qXmVUNmdoSZLch9DZwDPFHrcbSqav0iRVUEdc4LL/PMGXOqWspZrDxVWs9MqtikcvUoHz0OeMjMtgOPAZvd/c46jKMlVesXKerKwp1pZZ6FqnX2psVj6RPVp0rFAMmqeSBw92fd/ZeDP6e6++dqPYZWVq1fpKiD+asjo1y55jSOXTAn8rHVOnvT4rH0Wb2imyvXnEZ3ZwdGdm5AE8XJU/loi6mkHDRMsXr9U05YyIQ7C+fO4uDYBAeDzWWgumdvyhenU5wyYqkuBYIWVI1fpLB6fYChN0dZ87UfMG92O9/+2Cp2vPhqYqWcWjwmUhsKBBIqdzC//PYnGRwZnbw91zn09YNjnPPlBxl3p7MjQ+e8DC8NjkymbaoRDLR4TKQ21HROIq1e0c2hseiFY+PBGpTBkVFeGR6t+oSu8sUitaErAonU2z/A8OhE6TsWqHTdQhjli0WSpysCiTST6hxN6Io0DwUCiVSqlUQxmtAVaR5KDQkwtaNou9lk/r8SmtAVaS4KBDKtUV2pIDAv08bw6MRkwOjsyGAGg8Oj6gYq0oQUCFIiv4HckQUH7jcn+a05AAAKP0lEQVQOjhVtK53T2ZHh8vedqoO8SItRIEiBwjP+/HUB5cwDHDFnloKASAvSZHEKhPXsqYQqgURakwJBClTrAK5KIJHWpECQAqX2EMhZODebKWyz6T9TJZBI61IgaHG9/QO8HvQHKmV03Ln6/OU8e+V7ufr85WrtIJISmixucVdt2cXoRLw1AfmtIdTaQSQ9FAhaTOE+w+WuDtaEsEj6KBC0kMIy0UpaRLSZsWT95sQXhhUGLC1CE6kfBYIWUo0y0dyq4lw7aajO3gL5wgJWUq8lIqVpsriFlJvWWbX06MkJ4XabXiqU1P7A2otYpLHoiqCFlDMn0JFp48Y//fXJ75es3xx6vyTmDLQXsUhj0RVBCzlrWRchSwCm6ci0c+Wat0+5LWqxWBKLyGr5WiJSmgJBi/jwdY/wzUdfoFShaGdHJnRNwLpzT6Yj0z7ltqQWkdXytUSkNKWGmlj+HgJxRTWOy91Wi0qeWr6WiJRmPoMNSGqlp6fH+/r66j2MhlJYeROXAc9teG8ygxKRhmJmW929p9T9lBpqUpWWiraZ0ds/kMCIRKRZKRA0qUorbMbduWzTDgUDEZmkQNCk4nYUDVkeoJp9EZlCgaBJHSyRFpqXaePq85cTVUakmn0RyVHVUBP68HWPMDw6UfQ+P/m7dwNEVhUd2ZFh1Yb7VLUjIroiaCZ/3buDxes38/BPXy56v+68hVlhNfuZNuONQ2MMDI7gHO71o3kDkXSqSyAws/PMbJeZPWNm6+sxhmbz1707+OajL5S8X6bdpizMWr2imyvXnDZlk5n5c2cxOj41Z6R5A5H0qnlqyMzagf8N/DbwIvAjM7vd3X9S67E0kzhBAOCqP/jlaSmewk1matlXSEQaXz2uCE4HnnH3Z939EPAt4P11GEfL6ezIxMrzq9ePiOSrRyDoBv4t7/sXg9umMLO1ZtZnZn0HDhyo2eCa2eXvOzXW/dTrR0Ty1SMQhDXInFbk6O7XunuPu/d0dXXVYFjN7YKVi2JX/YTNG2hzepH0qkf56IvAiXnf/xzwUh3G0TR27xsq+vMLVi7is6tPK+s5tTm9iOTU44rgR8BJZrbEzGYDHwBur8M4msLufUN86LpHI38+yyg7CIiI5Kt5IHD3MeDjwBZgJ3CLuz9Z63E0g1wQaDPj3kveydz2qVm1ue3GM1eqk6iIzExdVha7+x3AHfV47WaRHwRuWruSpV3zeepz76n3sESkBWllcQMKCwIiIklRIGgwCgIiUmsKBA1EQUBE6kGBoEEoCIhIvSgQNAAFARGpp6bYvN7MDgB7aviSxwA/q+HrlUvjmxmNb+YafYwaX9Zb3b1ka4amCAS1ZmZ97t5T73FE0fhmRuObuUYfo8ZXHqWGRERSToFARCTlFAjCXVvvAZSg8c2MxjdzjT5Gja8MmiMQEUk5XRGIiKScAkEEM7vczAbMbFvwpyE6vpnZeWa2y8yeMbP19R5PITN73sx2BO9ZXwOM5xtmtt/Mnsi77Wgzu9vMng7+PqrBxtcwnz0zO9HM7jeznWb2pJldHNzeEO9hkfE1xHtoZnPN7DEz2x6M7zPB7UvM7IfB+3dz0JK/bpQaimBmlwOvu/sX6z2WHDNrB3YDv012g58fAR9095/UdWB5zOx5oMfdG6KG28x+E3gduMHdfym47QvAy+6+IQimR7n7pQ00vstpkM+emR0PHO/uPzazBcBWYDXwJzTAe1hkfH9EA7yHZmbAEe7+upllgIeAi4G/Aja5+7fM7OvAdne/pl7j1BVBczkdeMbdn3X3Q8C3gPfXeUwNzd0fBF4uuPn9wPXB19eTPXDURcT4Goa773X3HwdfD5HdQ6SbBnkPi4yvIXjW68G3meCPA2cD/xLcXtfPICgQlPJxM3s8uHyvW/ogTzfwb3nfv0gDfegDDtxlZlvNbG29BxPhOHffC9kDCXBsnccTptE+e5jZYmAF8EMa8D0sGB80yHtoZu1mtg3YD9wN/BQYDDbpggb4PU51IDCze8zsiZA/7weuAZYCy4G9wJfqOtgsC7mt0XJ7q9z9V4B3Ax8LUh9Snob77JnZfOA24JPu/lq9x1MoZHwN8x66+7i7Lye7P/vpwC+G3a22o5qqLjuUNQp3f1ec+5nZdcD3Eh5OHC8CJ+Z9/3PAS3UaSyh3fyn4e7+ZfZvsB//B+o5qmn1mdry77w1yzPvrPaB87r4v93UjfPaC3PZtwI3uvim4uWHew7DxNdp7CODug2b2ALAS6DSzWcFVQd1/j1N9RVBM8OHO+T3giaj71tCPgJOCioPZwAeA2+s8pklmdkQwYYeZHQGcQ2O8b4VuBz4afP1R4Dt1HMs0jfTZCyY7NwI73f3LeT9qiPcwanyN8h6aWZeZdQZfdwDvIjuPcT/wB8Hd6v4ZVNVQBDP7J7KXlQ48D1yUy4nWU1AGdzXQDnzD3T9X5yFNMrOfB74dfDsL+Od6j8/MbgLOJNvtcR/waaAXuAVYBLwA/KG712XCNmJ8Z9Ignz0zOwP4V2AHMBHc/Cmyefi6v4dFxvdBGuA9NLO3k50Mbid74n2Lu/9t8LvyLeBooB+4wN0P1np8k+NUIBARSTelhkREUk6BQEQk5RQIRERSToFARCTlFAhERFJOgUBahpm9Ja/b5P8r6D5Zle6OZrbAzP49WMmaf/v3zGxNkce9y8x6qzEGkWpL9cpiaS3u/u9ka8cjO3gGC5DM3SemP0Os1xgys/vINl27MXjOo4Bf4/ACIZGmoisCaXlm9ragh9TXgR8DJ5rZYN7PP2Bmfx98fZyZbTKzvqCP/MqQp7yJ7KrunN8HNrv7m2a20sweMbN+M3vYzE4KGc9nzeyTed8/ZWY/F3z90eB1t5nZ18yszcxmmdk/WXafhyfM7BPVeWdEshQIJC1OATa6+wpgoMj9/gfwBXfvIdvT/u9D7rMZWJnX0fIDZIMDZNsHnBG8zt8Bn407QDP7JbLtEH4jaFI2K3judwDHuPtpwZ4FN8R9TpE4lBqStPipu/8oxv3eBZyczSABcJSZdbj7SO4Gdz9oZpuBNWb2PeBU4N7gx53ADWa2tIIxvgv4VaAveP0Osm3HtwRj+ipwB3BXBc8tEkmBQNLijbyvJ5ja0ntu3tcGnB5s/FPMTcB/IXuw3pTXW/5zwBZ3/5qZvQ24M+SxY0y9Gs+9vpHtH/XfCx8Q9Kx5N/AJsqmoRt3rQZqQUkOSOsFE8StmdpKZtZFNx+TcA3ws942ZLY94mnvIXgn8GYfTQgBHcjj19CcRj32ebLoHMzudw63F7wH+yMyOCX72FjNbZGZdZCe4byXblO5XYvwzRWJTIJC0upTs2fq9ZPd5yPkYsCrY2eonwJ+GPdjdx8l2Wl0IPJz3o88DV5nZw2GPC9wKHGdm/cCFwLPBc+4APgPcY2aPk00BHUc2UDwY7HJ1HdnumiJVo+6jIiIppysCEZGUUyAQEUk5BQIRkZRTIBARSTkFAhGRlFMgEBFJOQUCEZGUUyAQEUm5/w/t8+vh0XylOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predictions = model.predict(X_test).flatten()\n",
    "\n",
    "plt.scatter(y_test[400:600],test_predictions[400:600])\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.axis('equal')\n",
    "plt.xlim(plt.xlim())\n",
    "plt.ylim(plt.ylim())\n",
    "_ = plt.plot([-100, 100], [-100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history.history['val_mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('../data/test.csv')\n",
    "test['month']=pd.to_datetime(test.Date,dayfirst=True).dt.month\n",
    "#test['Day']=pd.to_datetime(test.Date,dayfirst=True).dt.day\n",
    "test=test.drop(['Date','Time'],axis=1)\n",
    "test1=test['solar_output']\n",
    "test=test.drop('solar_output',axis=1)\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=test.mean(axis=0)\n",
    "std=test.std(axis=0)\n",
    "\n",
    "test=(test-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df['prediction']=test_predictions\n",
    "df.to_csv('prediction_march.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating with Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18270, 4)\n",
      "(18270,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y = sc.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=X.drop(['visibility'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X1, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y, test_size=0.3,random_state=40)\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>visibility</th>\n",
       "      <th>serial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14761</th>\n",
       "      <td>17.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>11.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15964</th>\n",
       "      <td>8.2</td>\n",
       "      <td>76.00</td>\n",
       "      <td>8.22</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>26.6</td>\n",
       "      <td>24.64</td>\n",
       "      <td>6.12</td>\n",
       "      <td>4.3</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>26.7</td>\n",
       "      <td>51.09</td>\n",
       "      <td>13.07</td>\n",
       "      <td>5.9</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19751</th>\n",
       "      <td>16.7</td>\n",
       "      <td>49.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       temperature  humidity  wind_speed  visibility  serial\n",
       "14761         17.0     20.00       11.10         5.0     117\n",
       "15964          8.2     76.00        8.22         3.0      15\n",
       "2787          26.6     24.64        6.12         4.3      33\n",
       "1630          26.7     51.09       13.07         5.9      36\n",
       "19751         16.7     49.27        0.00         2.1      32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_test = pd.read_csv('modified_test_interpolated.csv')\n",
    "#data_test=data_test.drop(['Date','Time'],axis=1)\n",
    "#X_test = data_test.iloc[:,1:]\n",
    "#Y = list(data.iloc[:,:1])\n",
    "#Y_test= np.asarray(data_test['solar_output'], dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test=pd.read_csv('test_interpolated.csv')\n",
    "#test=test.drop(['Date','Time','visibility','wind_speed'],axis=1)\n",
    "#X_TEST_NEW = test.iloc[:,1:]\n",
    "#Y_TEST_NEW= np.asarray(test['solar_output'], dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_NEW = rf.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93390808739603681"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Scores R2\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y1_test, pred_NEW) ## With Reduced Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Random Forest Regressor\n",
      "R2 on 10-fold CV: 0.9014\n"
     ]
    }
   ],
   "source": [
    "## Using K-FoldCross Validation\n",
    "from sklearn.cross_validation import KFold\n",
    "rf = RandomForestRegressor()\n",
    "X1=X1.values\n",
    "# Compute R2 using 10-fold x-validation\n",
    "kf = KFold(len(X), n_folds=10)\n",
    "xval_err = 0\n",
    "for train,test in kf:\n",
    "    rf.fit(X1[train],Y[train])\n",
    "    p = rf.predict(X1[test])\n",
    "    e = p-Y[test]\n",
    "    xval_err += r2_score(Y[test], p)\n",
    "rf_10cv = np.sqrt(xval_err/10)\n",
    "\n",
    "method_name = 'Random Forest Regressor'\n",
    "print('Method: %s' %method_name)\n",
    "\n",
    "print('R2 on 10-fold CV: %.4f' %rf_10cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('modified_test_interpolated.csv')\n",
    "data_test=data_test.drop(['Date','Time','visibility'],axis=1)\n",
    "X_test = data_test.iloc[:,1:]\n",
    "Y = list(data.iloc[:,:1])\n",
    "Y_test= np.asarray(data_test['solar_output'], dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre=rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ab=pd.DataFrame({'original':Y_test,'Prediction_withoutVisibility':pre})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ab.to_csv('abc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2 Scores using ALL FEATURE SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94307881621930378"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Old R2 Score With all the Feature Set\n",
    "rf = RandomForestRegressor()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,random_state=40)\n",
    "rf.fit(X_train, y_train)\n",
    "#test=pd.read_csv('test_interpolated.csv')\n",
    "#test=test.drop(['Date','Time'],axis=1)\n",
    "#X_TEST_OLD = test.iloc[:,1:]\n",
    "#Y = list(data.iloc[:,:1])\n",
    "#Y_TEST_OLD= np.asarray(test['solar_output'], dtype=\"int\")\n",
    "pred_OLD = rf.predict(X_test)\n",
    "r2_score(y_test, pred_OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Random Forest Regressor\n",
      "R2 on 10-fold CV: 0.9035\n"
     ]
    }
   ],
   "source": [
    "## Using K-FoldCross Validation\n",
    "from sklearn.cross_validation import KFold\n",
    "rf = RandomForestRegressor()\n",
    "X=X.values\n",
    "# Compute R2 using 10-fold x-validation\n",
    "kf = KFold(len(X), n_folds=10)\n",
    "xval_err = 0\n",
    "for train,test in kf:\n",
    "    rf.fit(X[train],Y[train])\n",
    "    p = rf.predict(X[test])\n",
    "    e = p-Y[test]\n",
    "    xval_err += r2_score(Y[test], p)\n",
    "rf_10cv = np.sqrt(xval_err/10)\n",
    "\n",
    "method_name = 'Random Forest Regressor'\n",
    "print('Method: %s' %method_name)\n",
    "\n",
    "print('R2 on 10-fold CV: %.4f' %rf_10cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'original':y_test, 'prediction':pred_OLD}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('new_result_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Regressor ( Before Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>serial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   temperature  humidity  serial\n",
       "0         27.0      74.0       1\n",
       "1         27.0      74.0       2\n",
       "2         27.0      74.0       3\n",
       "3         27.0      74.0       4\n",
       "4         27.0      74.0       5"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.drop(['Date','Time','visibility','wind_speed'],axis=1)\n",
    "X = data.iloc[:,1:]\n",
    "\n",
    "Y= np.asarray(data['solar_output'], dtype=\"int\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52775547,  1.60259432, -1.72014654],\n",
       "       [ 0.52775547,  1.60259432, -1.69625561],\n",
       "       [ 0.52775547,  1.60259432, -1.67236469],\n",
       "       ..., \n",
       "       [ 0.51194463, -0.3678876 ,  1.67236469],\n",
       "       [ 0.14829538, -0.13913151,  1.69625561],\n",
       "       [-0.12048886,  0.04325509,  1.72014654]])"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "#Y=sc.fit_transform(Y)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = MLPRegressor(\n",
    "    hidden_layer_sizes=(10,),  activation='relu', solver='adam', alpha=0.5, batch_size='auto',\n",
    "    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "    random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_neural_old = n.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 Score with all the Feature is 0.859044\n",
      "The Correlation between the Prediction and original Output Score Feature Selection is 0.926921\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, pred_neural_old)\n",
    "print(\"The R2 Score with all the Feature is %f\"  %r2_score(y_test, pred_neural_old))\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'original':y_test, 'prediction':pred_neural_old})\n",
    "print(\"The Correlation between the Prediction and original Output Score Feature Selection is %f\"  %df['original'].corr(df['prediction']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: MLP Regressor\n",
      "R2 on 10-fold CV: 0.9163\n"
     ]
    }
   ],
   "source": [
    "## K-FoldCross Validation\n",
    "from sklearn.cross_validation import KFold\n",
    "nn = MLPRegressor(\n",
    "    hidden_layer_sizes=(10,),  activation='relu', solver='adam', alpha=0.5, batch_size='auto',\n",
    "    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "    random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "# Compute R2 using 10-fold x-validation\n",
    "kf = KFold(len(X), n_folds=10)\n",
    "xval_err = 0\n",
    "for train,test in kf:\n",
    "    nn.fit(X[train],Y[train])\n",
    "    p = nn.predict(X[test])\n",
    "    e = p-Y[test]\n",
    "    xval_err += r2_score(Y[test], p)\n",
    "rf_10cv = np.sqrt(xval_err/10)\n",
    "\n",
    "method_name = 'MLP Regressor'\n",
    "print('Method: %s' %method_name)\n",
    "\n",
    "print('R2 on 10-fold CV: %.4f' %rf_10cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Regressor ( AfterFeature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>visibility</th>\n",
       "      <th>serial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   temperature  humidity  visibility  serial\n",
       "0         27.0      74.0         4.0       1\n",
       "1         27.0      74.0         4.0       2\n",
       "2         27.0      74.0         4.0       3\n",
       "3         27.0      74.0         4.0       4\n",
       "4         27.0      74.0         4.0       5"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train_data.csv')\n",
    "data=data.drop(['Date','Time','wind_speed'],axis=1)\n",
    "X = data.iloc[:,1:]\n",
    "Y= np.asarray(data['solar_output'], dtype=\"int\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52775547,  1.60259432,  0.20749581, -1.72014654],\n",
       "       [ 0.52775547,  1.60259432,  0.20749581, -1.69625561],\n",
       "       [ 0.52775547,  1.60259432,  0.20749581, -1.67236469],\n",
       "       ..., \n",
       "       [ 0.51194463, -0.3678876 , -0.21670963,  1.67236469],\n",
       "       [ 0.14829538, -0.13913151, -0.35811144,  1.69625561],\n",
       "       [-0.12048886,  0.04325509, -0.42881235,  1.72014654]])"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = MLPRegressor(\n",
    "    hidden_layer_sizes=(12,),  activation='relu', solver='adam', alpha=0.01, batch_size='auto',\n",
    "    learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "    random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_neural_old = n.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 Score After Feature Selection is 0.864121\n",
      "The Correlation between the Prediction and original Output Score Feature Selection is 0.929681\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(\"The R2 Score After Feature Selection is %f\"  %r2_score(y_test, pred_neural_old))\n",
    "\n",
    "df = pd.DataFrame({'original':y_test, 'prediction':pred_neural_old})\n",
    "print(\"The Correlation between the Prediction and original Output Score Feature Selection is %f\"  %df['original'].corr(df['prediction']))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52775547,  1.60259432,  0.20749581, -1.72014654],\n",
       "       [ 0.52775547,  1.60259432,  0.20749581, -1.69625561],\n",
       "       [ 0.52775547,  1.60259432,  0.20749581, -1.67236469],\n",
       "       ..., \n",
       "       [ 0.51194463, -0.3678876 , -0.21670963,  1.67236469],\n",
       "       [ 0.14829538, -0.13913151, -0.35811144,  1.69625561],\n",
       "       [-0.12048886,  0.04325509, -0.42881235,  1.72014654]])"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: MLP Regressor\n",
      "R2 on 10-fold CV: 0.9267\n"
     ]
    }
   ],
   "source": [
    "## K-FoldCross Validation\n",
    "#X=X.values\n",
    "from sklearn.cross_validation import KFold\n",
    "nn = MLPRegressor(\n",
    "    hidden_layer_sizes=(10,),  activation='relu', solver='adam', alpha=0.5, batch_size='auto',\n",
    "    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "    random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "# Compute R2 using 10-fold x-validation\n",
    "kf = KFold(len(X), n_folds=10)\n",
    "xval_err = 0\n",
    "for train,test in kf:\n",
    "    nn.fit(X[train],Y[train])\n",
    "    p = nn.predict(X[test])\n",
    "    e = p-Y[test]\n",
    "    xval_err += r2_score(Y[test], p)\n",
    "rf_10cv = np.sqrt(xval_err/10)\n",
    "\n",
    "method_name = 'MLP Regressor'\n",
    "print('Method: %s' %method_name)\n",
    "\n",
    "print('R2 on 10-fold CV: %.4f' %rf_10cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv('test_interpolated.csv')\n",
    "test=test.drop(['Date','Time','wind_speed'],axis=1)\n",
    "X = test.iloc[:,1:]\n",
    "Y= np.asarray(test['solar_output'], dtype=\"int\")\n",
    "X.head()\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "pred=nn.predict(X)\n",
    "df=pd.DataFrame({'original':Y,'prediction':pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'original':Y, 'prediction':pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('mlp_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_data.csv')\n",
    "data=data.drop(['Date','Time','wind_speed'],axis=1)\n",
    "X = data.iloc[:,1:]\n",
    "Y= np.asarray(data['solar_output'], dtype=\"int\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br=BaggingRegressor()\n",
    "br=br.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 Score After Feature Selection is 0.921388\n"
     ]
    }
   ],
   "source": [
    "pred=br.predict(X_test)\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"The R2 Score After Feature Selection is %f\"  %r2_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.DataFrame({'original':Y_test, 'prediction':pred})\n",
    "df['diff']=(df['prediction']-df['original'])\n",
    "#df.columns=['difference']\n",
    "#df['difference']=df[['original', 'prediction']].diff(axis=1)\n",
    "df['diff']=(df['diff']/df['original'])\n",
    "df['diff']=df['diff'].abs()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7823233330475436"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "\n",
    "rms = sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 22,  0, ...,  0,  0,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.44655833003\n"
     ]
    }
   ],
   "source": [
    "print(metrics.mean_absolute_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.213106090633\n"
     ]
    }
   ],
   "source": [
    "print metrics.accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>visibility</th>\n",
       "      <th>serial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>1.33</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>1.57</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>1.90</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>1.81</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>1.38</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>0.81</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>27.1</td>\n",
       "      <td>73.76</td>\n",
       "      <td>0.72</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>27.2</td>\n",
       "      <td>73.14</td>\n",
       "      <td>2.05</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27.4</td>\n",
       "      <td>72.30</td>\n",
       "      <td>3.69</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27.7</td>\n",
       "      <td>71.39</td>\n",
       "      <td>5.33</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>27.9</td>\n",
       "      <td>70.57</td>\n",
       "      <td>6.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>28.0</td>\n",
       "      <td>70.00</td>\n",
       "      <td>7.40</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28.0</td>\n",
       "      <td>69.77</td>\n",
       "      <td>7.72</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>28.0</td>\n",
       "      <td>69.79</td>\n",
       "      <td>8.05</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27.9</td>\n",
       "      <td>69.95</td>\n",
       "      <td>8.36</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>27.9</td>\n",
       "      <td>70.12</td>\n",
       "      <td>8.68</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>27.9</td>\n",
       "      <td>70.18</td>\n",
       "      <td>8.99</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>28.0</td>\n",
       "      <td>70.00</td>\n",
       "      <td>9.30</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>28.2</td>\n",
       "      <td>69.40</td>\n",
       "      <td>9.61</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28.3</td>\n",
       "      <td>68.41</td>\n",
       "      <td>9.91</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.5</td>\n",
       "      <td>67.18</td>\n",
       "      <td>10.22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28.7</td>\n",
       "      <td>65.92</td>\n",
       "      <td>10.52</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28.8</td>\n",
       "      <td>64.80</td>\n",
       "      <td>10.81</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25200</th>\n",
       "      <td>35.0</td>\n",
       "      <td>15.89</td>\n",
       "      <td>6.45</td>\n",
       "      <td>4.9</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25201</th>\n",
       "      <td>35.0</td>\n",
       "      <td>15.69</td>\n",
       "      <td>7.93</td>\n",
       "      <td>4.7</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25202</th>\n",
       "      <td>35.0</td>\n",
       "      <td>15.46</td>\n",
       "      <td>9.69</td>\n",
       "      <td>4.4</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25203</th>\n",
       "      <td>35.0</td>\n",
       "      <td>15.23</td>\n",
       "      <td>11.37</td>\n",
       "      <td>4.2</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25204</th>\n",
       "      <td>35.0</td>\n",
       "      <td>15.07</td>\n",
       "      <td>12.59</td>\n",
       "      <td>4.1</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25205</th>\n",
       "      <td>35.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25206</th>\n",
       "      <td>35.0</td>\n",
       "      <td>15.05</td>\n",
       "      <td>12.69</td>\n",
       "      <td>4.0</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25207</th>\n",
       "      <td>35.0</td>\n",
       "      <td>15.20</td>\n",
       "      <td>12.08</td>\n",
       "      <td>4.0</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25208</th>\n",
       "      <td>35.0</td>\n",
       "      <td>15.39</td>\n",
       "      <td>11.29</td>\n",
       "      <td>4.0</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25209</th>\n",
       "      <td>35.0</td>\n",
       "      <td>15.62</td>\n",
       "      <td>10.48</td>\n",
       "      <td>4.0</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25210</th>\n",
       "      <td>35.0</td>\n",
       "      <td>15.83</td>\n",
       "      <td>9.77</td>\n",
       "      <td>4.0</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25211</th>\n",
       "      <td>35.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>9.30</td>\n",
       "      <td>4.0</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25212</th>\n",
       "      <td>35.0</td>\n",
       "      <td>16.07</td>\n",
       "      <td>9.04</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25213</th>\n",
       "      <td>35.0</td>\n",
       "      <td>16.01</td>\n",
       "      <td>8.85</td>\n",
       "      <td>4.0</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25214</th>\n",
       "      <td>34.9</td>\n",
       "      <td>15.84</td>\n",
       "      <td>8.73</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25215</th>\n",
       "      <td>34.9</td>\n",
       "      <td>15.60</td>\n",
       "      <td>8.64</td>\n",
       "      <td>4.0</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25216</th>\n",
       "      <td>34.8</td>\n",
       "      <td>15.30</td>\n",
       "      <td>8.58</td>\n",
       "      <td>4.0</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25217</th>\n",
       "      <td>34.7</td>\n",
       "      <td>14.98</td>\n",
       "      <td>8.53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25218</th>\n",
       "      <td>34.6</td>\n",
       "      <td>14.66</td>\n",
       "      <td>8.47</td>\n",
       "      <td>4.0</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25219</th>\n",
       "      <td>34.5</td>\n",
       "      <td>14.37</td>\n",
       "      <td>8.38</td>\n",
       "      <td>4.0</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25220</th>\n",
       "      <td>34.4</td>\n",
       "      <td>14.13</td>\n",
       "      <td>8.25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25221</th>\n",
       "      <td>34.3</td>\n",
       "      <td>13.97</td>\n",
       "      <td>8.05</td>\n",
       "      <td>4.0</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25222</th>\n",
       "      <td>34.2</td>\n",
       "      <td>13.92</td>\n",
       "      <td>7.78</td>\n",
       "      <td>4.0</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25223</th>\n",
       "      <td>34.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>7.40</td>\n",
       "      <td>4.0</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25224</th>\n",
       "      <td>33.3</td>\n",
       "      <td>15.47</td>\n",
       "      <td>6.67</td>\n",
       "      <td>3.9</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25225</th>\n",
       "      <td>31.6</td>\n",
       "      <td>19.05</td>\n",
       "      <td>5.49</td>\n",
       "      <td>3.8</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25226</th>\n",
       "      <td>29.3</td>\n",
       "      <td>23.95</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.6</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25227</th>\n",
       "      <td>26.9</td>\n",
       "      <td>29.38</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.4</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25228</th>\n",
       "      <td>24.6</td>\n",
       "      <td>34.56</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3.2</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25229</th>\n",
       "      <td>22.9</td>\n",
       "      <td>38.69</td>\n",
       "      <td>0.36</td>\n",
       "      <td>3.1</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24835 rows Ã 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       temperature  humidity  wind_speed  visibility  serial\n",
       "0             27.0     74.00        0.00         4.0       1\n",
       "1             27.0     74.00        0.56         4.0       2\n",
       "2             27.0     74.00        1.00         4.0       3\n",
       "3             27.0     74.00        1.33         4.0       4\n",
       "4             27.0     74.00        1.57         4.0       5\n",
       "5             27.0     74.00        1.75         4.0       6\n",
       "6             27.0     74.00        1.90         4.0       7\n",
       "7             27.0     74.00        1.81         4.0       8\n",
       "8             27.0     74.00        1.38         4.0       9\n",
       "9             27.0     74.00        0.81         4.0      10\n",
       "10            27.0     74.00        0.27         4.0      11\n",
       "11            27.0     74.00        0.06         4.0      12\n",
       "12            27.0     74.00        0.00         4.0      13\n",
       "13            27.1     73.76        0.72         4.0      14\n",
       "14            27.2     73.14        2.05         4.0      15\n",
       "15            27.4     72.30        3.69         4.0      16\n",
       "16            27.7     71.39        5.33         4.0      17\n",
       "17            27.9     70.57        6.67         4.0      18\n",
       "18            28.0     70.00        7.40         4.0      19\n",
       "19            28.0     69.77        7.72         4.0      20\n",
       "20            28.0     69.79        8.05         4.0      21\n",
       "21            27.9     69.95        8.36         4.0      22\n",
       "22            27.9     70.12        8.68         4.0      23\n",
       "23            27.9     70.18        8.99         4.0      24\n",
       "24            28.0     70.00        9.30         4.0      25\n",
       "25            28.2     69.40        9.61         4.0      26\n",
       "26            28.3     68.41        9.91         4.0      27\n",
       "27            28.5     67.18       10.22         4.0      28\n",
       "28            28.7     65.92       10.52         4.0      29\n",
       "29            28.8     64.80       10.81         4.0      30\n",
       "...            ...       ...         ...         ...     ...\n",
       "25200         35.0     15.89        6.45         4.9     116\n",
       "25201         35.0     15.69        7.93         4.7     117\n",
       "25202         35.0     15.46        9.69         4.4     118\n",
       "25203         35.0     15.23       11.37         4.2     119\n",
       "25204         35.0     15.07       12.59         4.1     120\n",
       "25205         35.0     15.00       13.00         4.0     121\n",
       "25206         35.0     15.05       12.69         4.0     122\n",
       "25207         35.0     15.20       12.08         4.0     123\n",
       "25208         35.0     15.39       11.29         4.0     124\n",
       "25209         35.0     15.62       10.48         4.0     125\n",
       "25210         35.0     15.83        9.77         4.0     126\n",
       "25211         35.0     16.00        9.30         4.0     127\n",
       "25212         35.0     16.07        9.04         4.0     128\n",
       "25213         35.0     16.01        8.85         4.0     129\n",
       "25214         34.9     15.84        8.73         4.0     130\n",
       "25215         34.9     15.60        8.64         4.0     131\n",
       "25216         34.8     15.30        8.58         4.0     132\n",
       "25217         34.7     14.98        8.53         4.0     133\n",
       "25218         34.6     14.66        8.47         4.0     134\n",
       "25219         34.5     14.37        8.38         4.0     135\n",
       "25220         34.4     14.13        8.25         4.0     136\n",
       "25221         34.3     13.97        8.05         4.0     137\n",
       "25222         34.2     13.92        7.78         4.0     138\n",
       "25223         34.0     14.00        7.40         4.0     139\n",
       "25224         33.3     15.47        6.67         3.9     140\n",
       "25225         31.6     19.05        5.49         3.8     141\n",
       "25226         29.3     23.95        4.07         3.6     142\n",
       "25227         26.9     29.38        2.60         3.4     143\n",
       "25228         24.6     34.56        1.29         3.2     144\n",
       "25229         22.9     38.69        0.36         3.1     145\n",
       "\n",
       "[24835 rows x 5 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector=(X['temperature']>10)\n",
    "X.loc[selector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test.to_csv('abc1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ae080dfe78ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'abc_check.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv('abc_check.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>prediction</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.090900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.198160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.332000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.487540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.668560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.865750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.081800</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.630246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.332300</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.474593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>8.146620</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.472174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.620620</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.475108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.060020</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.419265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>6.473080</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.505645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>5.889250</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.473617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>5.449810</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.486221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>3.196630</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.468190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2.400040</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.750004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2.050580</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.902467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1.717740</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.941784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1.413910</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.929274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1.148090</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.825798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.009160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.101850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.214900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.354890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.514850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.698080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.898170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.122370</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.732708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1.373420</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.417512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>0.012010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>0.108180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>0.227420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>0.375790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>0.563060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>0.754460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>0.964940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>1.182450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>1.465180</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.453992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>1.746420</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.484660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>1.989800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.497437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>2.311630</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.437626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>6.428720</td>\n",
       "      <td>15.7</td>\n",
       "      <td>1.442166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>20.096130</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.666602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>19.829559</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.626820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>19.726971</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.543772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>18.661690</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.469501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>18.312380</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.481225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>18.036390</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.467743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>17.262430</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.438086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>16.665779</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.519974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>16.019991</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.581773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>15.420200</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.604415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>15.322150</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.517039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>14.991710</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.439690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>4.161300</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.423257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>3.622170</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.530668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>3.191170</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.467280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>2.755460</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.455626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>2.353230</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.447568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows Ã 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      original  prediction      diff\n",
       "4     0.007030         0.0  1.000000\n",
       "5     0.090900         0.0  1.000000\n",
       "6     0.198160         0.0  1.000000\n",
       "7     0.332000         0.0  1.000000\n",
       "8     0.487540         0.0  1.000000\n",
       "9     0.668560         0.0  1.000000\n",
       "10    0.865750         0.0  1.000000\n",
       "11    1.081800         0.4  0.630246\n",
       "12    1.332300         0.7  0.474593\n",
       "128   8.146620         4.3  0.472174\n",
       "129   7.620620         4.0  0.475108\n",
       "130   7.060020         4.1  0.419265\n",
       "131   6.473080         3.2  0.505645\n",
       "132   5.889250         3.1  0.473617\n",
       "133   5.449810         2.8  0.486221\n",
       "138   3.196630         1.7  0.468190\n",
       "140   2.400040         0.6  0.750004\n",
       "141   2.050580         0.2  0.902467\n",
       "142   1.717740         0.1  0.941784\n",
       "143   1.413910         0.1  0.929274\n",
       "144   1.148090         0.2  0.825798\n",
       "149   0.009160         0.0  1.000000\n",
       "150   0.101850         0.0  1.000000\n",
       "151   0.214900         0.0  1.000000\n",
       "152   0.354890         0.0  1.000000\n",
       "153   0.514850         0.0  1.000000\n",
       "154   0.698080         0.0  1.000000\n",
       "155   0.898170         0.0  1.000000\n",
       "156   1.122370         0.3  0.732708\n",
       "157   1.373420         0.8  0.417512\n",
       "..         ...         ...       ...\n",
       "728   0.012010         0.0  1.000000\n",
       "729   0.108180         0.0  1.000000\n",
       "730   0.227420         0.0  1.000000\n",
       "731   0.375790         0.0  1.000000\n",
       "732   0.563060         0.0  1.000000\n",
       "733   0.754460         0.0  1.000000\n",
       "734   0.964940         0.0  1.000000\n",
       "735   1.182450         0.0  1.000000\n",
       "736   1.465180         0.8  0.453992\n",
       "737   1.746420         0.9  0.484660\n",
       "738   1.989800         1.0  0.497437\n",
       "739   2.311630         1.3  0.437626\n",
       "769   6.428720        15.7  1.442166\n",
       "827  20.096130         6.7  0.666602\n",
       "828  19.829559         7.4  0.626820\n",
       "829  19.726971         9.0  0.543772\n",
       "832  18.661690         9.9  0.469501\n",
       "833  18.312380         9.5  0.481225\n",
       "834  18.036390         9.6  0.467743\n",
       "837  17.262430         9.7  0.438086\n",
       "838  16.665779         8.0  0.519974\n",
       "839  16.019991         6.7  0.581773\n",
       "840  15.420200         6.1  0.604415\n",
       "841  15.322150         7.4  0.517039\n",
       "842  14.991710         8.4  0.439690\n",
       "863   4.161300         2.4  0.423257\n",
       "864   3.622170         1.7  0.530668\n",
       "865   3.191170         1.7  0.467280\n",
       "866   2.755460         1.5  0.455626\n",
       "867   2.353230         1.3  0.447568\n",
       "\n",
       "[136 rows x 3 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['diff']>0.40,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'Result on Interpolated day of 5Days.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-79f90076f44a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0manalysis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Result on Interpolated day of 5Days.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'Result on Interpolated day of 5Days.csv' does not exist"
     ]
    }
   ],
   "source": [
    "analysis=pd.read_csv('Result on Interpolated day of 5Days.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c349f4e4b414>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0manalysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'analysis' is not defined"
     ]
    }
   ],
   "source": [
    "analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analysis=analysis.drop(['Unnamed: 7','Unnamed: 12','Unnamed: 13','Unnamed: 14'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>visibility</th>\n",
       "      <th>serial</th>\n",
       "      <th>S.No</th>\n",
       "      <th>original</th>\n",
       "      <th>prediction</th>\n",
       "      <th>ErrorRatio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25-03-17</td>\n",
       "      <td>6:00</td>\n",
       "      <td>22.0</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25-03-17</td>\n",
       "      <td>6:05</td>\n",
       "      <td>21.8</td>\n",
       "      <td>41.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25-03-17</td>\n",
       "      <td>6:10</td>\n",
       "      <td>21.6</td>\n",
       "      <td>42.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25-03-17</td>\n",
       "      <td>6:15</td>\n",
       "      <td>21.6</td>\n",
       "      <td>42.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25-03-17</td>\n",
       "      <td>6:20</td>\n",
       "      <td>21.6</td>\n",
       "      <td>42.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date  Time  temperature  humidity  wind_speed  visibility  serial  \\\n",
       "0  25-03-17  6:00         22.0     41.00         0.0         3.0       1   \n",
       "1  25-03-17  6:05         21.8     41.92         0.0         3.0       2   \n",
       "2  25-03-17  6:10         21.6     42.45         0.0         3.0       3   \n",
       "3  25-03-17  6:15         21.6     42.61         0.0         3.0       4   \n",
       "4  25-03-17  6:20         21.6     42.40         0.0         3.0       5   \n",
       "\n",
       "   S.No  original  prediction  ErrorRatio  \n",
       "0     0   0.00000         0.0         0.0  \n",
       "1     1   0.00000         0.0         0.0  \n",
       "2     2   0.00000         0.0         0.0  \n",
       "3     3   0.00000         0.0         0.0  \n",
       "4     4   0.00703         0.0         1.0  "
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1=analysis.loc[analysis.ErrorRatio>.30,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-dca3df960c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a1' is not defined"
     ]
    }
   ],
   "source": [
    "a1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date           245\n",
       "Time           245\n",
       "temperature    245\n",
       "humidity       245\n",
       "wind_speed     245\n",
       "visibility     245\n",
       "serial         245\n",
       "S.No           245\n",
       "original       245\n",
       "prediction     245\n",
       "ErrorRatio     245\n",
       "dtype: int64"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>visibility</th>\n",
       "      <th>serial</th>\n",
       "      <th>S.No</th>\n",
       "      <th>original</th>\n",
       "      <th>prediction</th>\n",
       "      <th>ErrorRatio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>25-03-17</td>\n",
       "      <td>10:05</td>\n",
       "      <td>30.2</td>\n",
       "      <td>22.49</td>\n",
       "      <td>7.23</td>\n",
       "      <td>3.1</td>\n",
       "      <td>50</td>\n",
       "      <td>49</td>\n",
       "      <td>17.949680</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.030659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>26-03-17</td>\n",
       "      <td>10:05</td>\n",
       "      <td>31.2</td>\n",
       "      <td>23.67</td>\n",
       "      <td>1.69</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50</td>\n",
       "      <td>194</td>\n",
       "      <td>17.386761</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.200541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>27-03-17</td>\n",
       "      <td>10:05</td>\n",
       "      <td>33.2</td>\n",
       "      <td>22.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50</td>\n",
       "      <td>339</td>\n",
       "      <td>17.517879</td>\n",
       "      <td>12.9</td>\n",
       "      <td>0.263610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>28-03-17</td>\n",
       "      <td>10:05</td>\n",
       "      <td>34.2</td>\n",
       "      <td>19.57</td>\n",
       "      <td>6.07</td>\n",
       "      <td>4.0</td>\n",
       "      <td>50</td>\n",
       "      <td>484</td>\n",
       "      <td>17.499420</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.085684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>29-03-17</td>\n",
       "      <td>10:05</td>\n",
       "      <td>34.8</td>\n",
       "      <td>17.82</td>\n",
       "      <td>14.75</td>\n",
       "      <td>4.0</td>\n",
       "      <td>50</td>\n",
       "      <td>629</td>\n",
       "      <td>12.130120</td>\n",
       "      <td>17.7</td>\n",
       "      <td>0.459178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>30-03-17</td>\n",
       "      <td>10:05</td>\n",
       "      <td>34.9</td>\n",
       "      <td>15.95</td>\n",
       "      <td>9.34</td>\n",
       "      <td>4.5</td>\n",
       "      <td>50</td>\n",
       "      <td>774</td>\n",
       "      <td>18.960979</td>\n",
       "      <td>17.6</td>\n",
       "      <td>0.071778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date   Time  temperature  humidity  wind_speed  visibility  serial  \\\n",
       "49   25-03-17  10:05         30.2     22.49        7.23         3.1      50   \n",
       "194  26-03-17  10:05         31.2     23.67        1.69         3.0      50   \n",
       "339  27-03-17  10:05         33.2     22.62        0.00         3.0      50   \n",
       "484  28-03-17  10:05         34.2     19.57        6.07         4.0      50   \n",
       "629  29-03-17  10:05         34.8     17.82       14.75         4.0      50   \n",
       "774  30-03-17  10:05         34.9     15.95        9.34         4.5      50   \n",
       "\n",
       "     S.No   original  prediction  ErrorRatio  \n",
       "49     49  17.949680        18.5    0.030659  \n",
       "194   194  17.386761        13.9    0.200541  \n",
       "339   339  17.517879        12.9    0.263610  \n",
       "484   484  17.499420        16.0    0.085684  \n",
       "629   629  12.130120        17.7    0.459178  \n",
       "774   774  18.960979        17.6    0.071778  "
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.loc[analysis.serial==50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>visibility</th>\n",
       "      <th>serial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.8</td>\n",
       "      <td>41.92</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.6</td>\n",
       "      <td>42.45</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.6</td>\n",
       "      <td>42.61</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.6</td>\n",
       "      <td>42.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   temperature  humidity  wind_speed  visibility  serial\n",
       "0         22.0     41.00         0.4         3.0       1\n",
       "1         21.8     41.92         0.4         3.0       2\n",
       "2         21.6     42.45         0.4         3.0       3\n",
       "3         21.6     42.61         0.4         3.0       4\n",
       "4         21.6     42.40         0.4         3.0       5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.55077801804016391"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['humidity'].corr(X['visibility'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34502025627017363"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['wind_speed'].corr(X['visibility'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59358314396419587"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['temperature'].corr(X['visibility'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45774914475989709"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['serial'].corr(X['visibility'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13797609199101929"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "Y.corr(X['serial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
